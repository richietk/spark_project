{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we get extra demographics data to improve out predictions and ensure we accurate get the feature importance of spending_data. Obviously, this isn't a foolproof method, but it is better than only using the spending data. New demographics data can be added by modifying the script.\n",
    "\n",
    "Currently we will use demographical data on nationality, gender, age, and highest completed education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: data/OOE_Bev_Staatsangehoerigkeit.csv\n",
      "Downloaded: data/OOE_Bev_laut_Volkszaehlung_Geschl_Alt5J.csv\n",
      "Downloaded: data/OOE_Bev_Hoechste_abgeschl_Ausbildung.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "\"\"\"\n",
    "URLs for the data.gv.at pages:\n",
    "Date accessed: 2/10/2025 5:41PM\n",
    "\n",
    "Bevölkerung - Staatsangehörigkeit\n",
    "https://www.data.gv.at/katalog/dataset/2a654cd4-60e2-4dc9-ba8d-f7d48343ecf6\n",
    "Bevölkerung - Geschlecht und Alter laut Volkszählung\n",
    "https://www.data.gv.at/katalog/dataset/076e497b-e2e2-409f-a32c-009e05c5f957\n",
    "Bevölkerung - Höchste abgeschlossene Ausbildung\n",
    "https://www.data.gv.at/katalog/dataset/b80ecd53-cfe8-4c9f-9c47-ee567ce94f45\n",
    "\n",
    "The download urls can be found on the [URL]#resources tab\n",
    "\"\"\"\n",
    "\n",
    "urls = [\n",
    "    \"https://e-gov.ooe.gv.at/at.gv.ooe.ogd2-citi/api/file/VAQ5Lkz1NttM9P4OQBxqaw/OOE_Bev_Staatsangehoerigkeit.csv\",\n",
    "    \"https://e-gov.ooe.gv.at/at.gv.ooe.ogd2-citi/api/file/tT_GXC5wGdPvjNfN3GD1ow/OOE_Bev_laut_Volkszaehlung_Geschl_Alt5J.csv\",\n",
    "    \"https://e-gov.ooe.gv.at/at.gv.ooe.ogd2-citi/api/file/E6urCdCJP4L-ZRbWAhzkiA/OOE_Bev_Hoechste_abgeschl_Ausbildung.csv\"\n",
    "]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "for url in urls:\n",
    "    try:\n",
    "        filename = os.path.join(\"data\", url.split(\"/\")[-1])\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a scalable pipeline for processing election and demographic data using Apache Spark. We will use Spark DataFrames to handle large datasets efficiently and train a Random Forest classifier to predict the winning party in elections.\n",
    "\n",
    "At first, we initialize spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import broadcast, udf, when, col\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, OneVsRest\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from unidecode import unidecode # https://pypi.org/project/Unidecode/\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MunicipalSpendingAndElectionAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the existing merged data that we created in `etl.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data/merged_data.csv\n",
      "First 5 rows now after adding id (and casting Year):\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+-------------------+---------------------+-----------------+-----------------------+---+\n",
      "|Municipality_Lowercase|Year|Municipality_ID|Municipality_Name|Wahlberechtigte|abgegebene_Stimmen|gueltige_Stimmen|Wahlbeteiligung|Winning_Party|Spending_Summe     |Education_Spending_PC|Total_Spending_PC|Edu_Spending_Percentage|id |\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+-------------------+---------------------+-----------------+-----------------------+---+\n",
      "|linz                  |2008|40101          |Linz             |142125         |96209             |94496           |67.69          |SPO          |1.203534975E8      |319.61816451293      |3378.0490982967  |9.461619864379408      |0  |\n",
      "|steyr                 |2008|40201          |Steyr            |28962          |20765             |20335           |71.7           |SPO          |3.377888836328125E7|436.81479839648      |3132.9064381223  |13.942797431840445     |1  |\n",
      "|wels                  |2008|40301          |Wels             |40994          |28803             |28288           |70.26          |SPO          |5.50799846875E7    |471.85801422085      |3152.7184429024  |14.966703267877495     |2  |\n",
      "|aspach                |2008|40402          |Aspach           |1869           |1390              |1346            |74.37          |OVP          |1433794.4849853516 |306.75962772786      |2834.8965554129  |10.82084025754445      |3  |\n",
      "|auerbach              |2008|40403          |Auerbach         |411            |310               |301             |75.43          |OVP          |160001.2024230957  |152.3820952381       |2021.9030666667  |7.536567788549646      |4  |\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+-------------------+---------------------+-----------------+-----------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data_path = \"data/merged_data.csv\"\n",
    "merged_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .option(\"sep\", \",\")\n",
    "         .option(\"enforceSchema\", True)\n",
    "         .csv(merged_data_path)\n",
    "         .drop(\"_c0\")  # Remove unnecessary `_c0` index col if appears\n",
    ")\n",
    "\n",
    "print(f\"Loaded {merged_data_path}\")\n",
    "\n",
    "# To join later on demographic data we add a unique ID.\n",
    "merged_df = merged_df.withColumn(\"id\", F.monotonically_increasing_id()) \\\n",
    "                     .withColumn(\"Year\", F.col(\"Year\").cast(\"integer\")) # ensure yr is int\n",
    "\n",
    "print(\"First 5 rows now after adding id (and casting Year):\")\n",
    "merged_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load our demographics data.\n",
    "\n",
    "We calculate the ratio of Austrians per total population, the ratio of university graduates per total pop., and the ratio of 65+ population amongst the total population. If we want to use other ratios, we can adjust the respective part of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from Nationality data:\n",
      "+----------------------+---------+------------------+\n",
      "|Municipality_Lowercase|orig_year|Austria_Ratio     |\n",
      "+----------------------+---------+------------------+\n",
      "|linz                  |2023     |0.7072245498811006|\n",
      "|steyr                 |2023     |0.7656833359625598|\n",
      "|wels                  |2023     |0.6647265152327416|\n",
      "+----------------------+---------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Sample rows from Education Level data:\n",
      "+----------------------+---------+-------------------+\n",
      "|Municipality_Lowercase|orig_year|Uni_Grad_Ratio     |\n",
      "+----------------------+---------+-------------------+\n",
      "|linz                  |2022     |0.15113934789550695|\n",
      "|steyr                 |2022     |0.08382775624291328|\n",
      "|wels                  |2022     |0.08342151126602763|\n",
      "+----------------------+---------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Sample rows from Age data:\n",
      "+----------------------------+---------+-------------------+\n",
      "|Municipality_Lowercase      |orig_year|Pop_65plus_Ratio   |\n",
      "+----------------------------+---------+-------------------+\n",
      "|st.-georgen-am-fillmannsbach|2011     |0.14824120603015076|\n",
      "|diersbach                   |2011     |0.161090458488228  |\n",
      "|stadl-paura                 |2001     |0.15639128647759967|\n",
      "+----------------------------+---------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def add_muni_col(df, source_col):\n",
    "    \"\"\"\n",
    "    helper function to create a \"Municipality_Lowercase\" col from a source col\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        \"Municipality_Lowercase\",\n",
    "        F.regexp_replace(F.lower(F.col(source_col)), \" \", \"-\")\n",
    "    )\n",
    "\n",
    "# --- A) NATIONALITY (Staatsangehoerigkeit) ---\n",
    "nation_path = \"data/OOE_Bev_Staatsangehoerigkeit.csv\"\n",
    "df_nation = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"sep\", \";\")\n",
    "         .option(\"encoding\", \"latin1\")\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(nation_path)\n",
    ")\n",
    "df_nation = add_muni_col(df_nation, \"LAU2_NAME\") # using helper. LAU2 = municipality name\n",
    "# rename cols for clarity\n",
    "df_nation = df_nation.withColumnRenamed(\"YEAR\", \"orig_year\") \\\n",
    "                     .withColumnRenamed(\"NATION_AUSTRIA\", \"Nation_Austria\") \\\n",
    "                     .withColumnRenamed(\"NATION_TOTAL\", \"Nation_Total\")\n",
    "# cast to numeric (float) as needed\n",
    "df_nation = df_nation.withColumn(\"Nation_Austria\", F.col(\"Nation_Austria\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Nation_Total\", F.col(\"Nation_Total\").cast(\"float\")) \\\n",
    "                     .withColumn(\"orig_year\", F.col(\"orig_year\").cast(\"integer\"))\n",
    "\n",
    "# calculate ratio of austrians per population per municipality\n",
    "df_nation = df_nation.withColumn(\"Austria_Ratio\", F.col(\"Nation_Austria\") / F.col(\"Nation_Total\"))\n",
    "df_nation = df_nation.select(\"Municipality_Lowercase\", \"orig_year\", \"Austria_Ratio\")\n",
    "\n",
    "print(\"Sample rows from Nationality data:\")\n",
    "df_nation.show(3, truncate=False)\n",
    "\n",
    "# --- B) EDUCATION LEVEL (Höchste_abgeschl_Ausbildung) ---\n",
    "edu_path = \"data/OOE_Bev_Hoechste_abgeschl_Ausbildung.csv\"\n",
    "df_edu = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"sep\", \";\")\n",
    "         .option(\"encoding\", \"latin1\")\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(edu_path)\n",
    ")\n",
    "df_edu = add_muni_col(df_edu, \"COMMUNE_NAME\") # using helper\n",
    "# rename cols for clarity\n",
    "df_edu = df_edu.withColumnRenamed(\"YEAR\", \"orig_year\") \\\n",
    "               .withColumnRenamed(\"EDU_UNIVERSITY_FACHHOCHSCHULE\", \"Uni_Grads\") \\\n",
    "               .withColumnRenamed(\"EDU_TOTAL\", \"Edu_Total\")\n",
    "# cast to numeric (float) as needed\n",
    "df_edu = df_edu.withColumn(\"Uni_Grads\", F.col(\"Uni_Grads\").cast(\"float\")) \\\n",
    "               .withColumn(\"Edu_Total\", F.col(\"Edu_Total\").cast(\"float\")) \\\n",
    "               .withColumn(\"orig_year\", F.col(\"orig_year\").cast(\"integer\"))\n",
    "\n",
    "# calculate ratio of uni grads per population per municipality\n",
    "df_edu = df_edu.withColumn(\"Uni_Grad_Ratio\", F.col(\"Uni_Grads\") / F.col(\"Edu_Total\"))\n",
    "df_edu = df_edu.select(\"Municipality_Lowercase\", \"orig_year\", \"Uni_Grad_Ratio\")\n",
    "\n",
    "print(\"Sample rows from Education Level data:\")\n",
    "df_edu.show(3, truncate=False)\n",
    "\n",
    "# --- C) AGE / POPULATION (Bevölkerung laut Volkszählung) ---\n",
    "age_path = \"data/OOE_Bev_laut_Volkszaehlung_Geschl_Alt5J.csv\"\n",
    "df_age = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"sep\", \";\")\n",
    "         .option(\"encoding\", \"latin1\")\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(age_path)\n",
    ")\n",
    "df_age = add_muni_col(df_age, \"LAU2_NAME\") # using helper\n",
    "\n",
    "# For our analysis, we will calculate the ratio of 65+ / total population.\n",
    "# If we want to do some other ratio, adjust this code.\n",
    "\n",
    "# columns for 65+\n",
    "age_cols_65plus = [\n",
    "    \"AGE_65_TO_69\", \"AGE_70_TO_74\", \"AGE_75_TO_79\",\n",
    "    \"AGE_80_TO_84\", \"AGE_85_TO_89\", \"AGE_90_PLUS\"\n",
    "]\n",
    "\n",
    "# ensure correct types\n",
    "for c in age_cols_65plus:\n",
    "    df_age = df_age.withColumn(c, F.when(F.col(c).isNull(), 0)\n",
    "                                       .otherwise(F.col(c).cast(\"float\")))\n",
    "df_age = df_age.withColumn(\"AGE_TOTAL\", F.when(F.col(\"AGE_TOTAL\").isNull(), 0)\n",
    "                                         .otherwise(F.col(\"AGE_TOTAL\").cast(\"float\")))\n",
    "# create row-wise sum of the 65+ population\n",
    "df_age = df_age.withColumn(\"POP_65plus\", sum([F.col(c) for c in age_cols_65plus]))\n",
    "df_age = df_age.withColumnRenamed(\"YEAR\", \"orig_year\") \\\n",
    "               .withColumn(\"orig_year\", F.col(\"orig_year\").cast(\"integer\"))\n",
    "\n",
    "# group by municipality and orig_year\n",
    "df_age = df_age.groupBy(\"Municipality_Lowercase\", \"orig_year\") \\\n",
    "               .agg(\n",
    "                   F.sum(\"POP_65plus\").alias(\"POP_65plus\"),\n",
    "                   F.sum(\"AGE_TOTAL\").alias(\"AGE_TOTAL\")\n",
    "               )\n",
    "df_age = df_age.withColumn(\"Pop_65plus_Ratio\", F.col(\"POP_65plus\") / F.col(\"AGE_TOTAL\"))\n",
    "df_age = df_age.select(\"Municipality_Lowercase\", \"orig_year\", \"Pop_65plus_Ratio\")\n",
    "\n",
    "print(\"Sample rows from Age data:\")\n",
    "df_age.show(3, truncate=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240, 14)\n",
      "(4818, 3)\n",
      "(3066, 3)\n",
      "(2628, 3)\n"
     ]
    }
   ],
   "source": [
    "# dimensions of dataframes\n",
    "print((merged_df.count(), len(merged_df.columns)))\n",
    "print((df_nation.count(), len(df_nation.columns)))\n",
    "print((df_edu.count(), len(df_edu.columns)))\n",
    "print((df_age.count(), len(df_age.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, since demographic data isn't available for every election year, we match each row with closest (in terms of date) available demographic data.\n",
    "\n",
    "While this isn't a perfect solution, it is good enough for our purposes. There are more accurate solutions (e.g. extrapolating data), but for the sake of scalability, and for avoiding unnecessary complexity, we will not do such methods. The time complexity from the initial join is roughly O(N), where N is the number of rows in the main merged dataset.\n",
    "\n",
    "This efficiency is achieved by broadcasting the relatively small demographic datasets, which we will use to join the closest nationality, education, and age ratios to the election data. We will assume the nation, edu, and age dataset fits into memory, so we will use a broadcast join. Thus, the join operation essentially scales linearly with N. The subsequent window function (which orders demographic records per unique id) contributes only a constant factor, keeping the overall complexity at O(N). If the demographic data does not fit into memory, the code can be modified to use a standard left join (O(N log N) due to sorting) or an alternative method like a hash join (O(N) when feasible).\n",
    "\n",
    "Examples of methods if demographic data does not fit into memory:\n",
    "```\n",
    "# left join\n",
    "joined = merged.join(demo_df, on=\"Municipality_Lowercase\", how=\"left\")\n",
    "\n",
    "# disable auto-broadcasting and perform a hash join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "joined = merged.join(demo_df.hint(\"shuffle_hash\"), on=\"Municipality_Lowercase\", how=\"left\")\n",
    "```\n",
    "Broadcast join docs: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of nation_closest join results:\n",
      "+---+------------------+\n",
      "|id |Austria_Ratio     |\n",
      "+---+------------------+\n",
      "|26 |0.9342235410484668|\n",
      "|29 |0.9445953286257469|\n",
      "|474|0.9523026315789473|\n",
      "+---+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Sample of edu_closest join results:\n",
      "+---+--------------------+\n",
      "|id |Uni_Grad_Ratio      |\n",
      "+---+--------------------+\n",
      "|26 |0.025025025025025027|\n",
      "|29 |0.05830849478390462 |\n",
      "|474|0.03536977491961415 |\n",
      "+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Sample of age_closest join results:\n",
      "+---+-------------------+\n",
      "|id |Pop_65plus_Ratio   |\n",
      "+---+-------------------+\n",
      "|26 |0.15015015015015015|\n",
      "|29 |0.17343517138599107|\n",
      "|474|0.19614147909967847|\n",
      "+---+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Row count after joining demographics: 1240\n",
      "Sample merged_df after joining demographics:\n",
      "+---+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+------------------+---------------------+-----------------+-----------------------+------------------+--------------------+-------------------+\n",
      "|id |Municipality_Lowercase|Year|Municipality_ID|Municipality_Name|Wahlberechtigte|abgegebene_Stimmen|gueltige_Stimmen|Wahlbeteiligung|Winning_Party|Spending_Summe    |Education_Spending_PC|Total_Spending_PC|Edu_Spending_Percentage|Austria_Ratio     |Uni_Grad_Ratio      |Pop_65plus_Ratio   |\n",
      "+---+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+------------------+---------------------+-----------------+-----------------------+------------------+--------------------+-------------------+\n",
      "|26 |tarsdorf              |2008|40443          |Tarsdorf         |1578           |1062              |1010            |67.3           |OVP          |1241949.890625    |310.33231384308      |2207.8512093953  |14.055852700693347     |0.9342235410484668|0.025025025025025027|0.15015015015015015|\n",
      "|29 |alkoven               |2008|40501          |Alkoven          |4224           |3360              |3276            |79.55          |SPO          |1852952.8623046875|175.66864050057      |1997.0596037163  |8.796364423659199      |0.9445953286257469|0.05830849478390462 |0.17343517138599107|\n",
      "|474|weilbach              |2013|41235          |Weilbach         |473            |360               |353             |76.11          |OVP          |444505.93994140625|353.343354531        |2272.1790620032  |15.550858664258843     |0.9523026315789473|0.03536977491961415 |0.19614147909967847|\n",
      "|964|pupping               |2019|40509          |Pupping          |1455           |981               |971             |67.42          |OVP          |847619.1716403961 |233.37531938326      |2594.0902973568  |8.996422353572404      |0.9467190128996074|0.06543075245365322 |0.20065430752453653|\n",
      "|65 |grnau-im-almtal       |2008|40707          |Grnau im Almtal  |1665           |1316              |1271            |79.04          |SPO          |639171.8681640625 |149.33921962617      |2845.0468831776  |5.249095208560315      |null              |null                |null               |\n",
      "+---+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+------------------+---------------------+-----------------+-----------------------+------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def closest_demographic_join(merged, demo_df, ratio_col):\n",
    "    \"\"\"\n",
    "    Joins demographic data to the main (merged) dataset by selecting the closest available year \n",
    "    for each municipality.\n",
    "\n",
    "    The function broadcasts the demographic dataset to optimize join performance.\n",
    "    Performs a left join on `Municipality_Lowercase` to merge demographic data.\n",
    "    Computes the absolute difference between `Year` (from `merged`) and `orig_year` (from `demo_df`).\n",
    "    Uses a window function to rank rows based on the closest year difference, \n",
    "        with ties resolved by preferring the most recent `orig_year`.\n",
    "    Selects the best match (i.e., the row with rank 1) and returns a DataFrame \n",
    "       containing `id` and the requested demographic ratio column.\n",
    "\n",
    "    :param merged: pyspark.sql.DataFrame election/spendings dataset w/ \n",
    "        The main DataFrame containing the primary dataset with a \"Year\" col.\n",
    "    :param demo_df: pyspark.sql.DataFrame The demographic dataset containing \"Municipality_Lowercase, \"orig_year\", and the ratio col.\n",
    "    ratio_col : str Name of the col in demo_df that contains the demographic ratio to be joined.\n",
    "\n",
    "    :return: pyspark.sql.DataFrame of \"id\" and \"ratio col\"\n",
    "    \n",
    "    Note:\n",
    "    - assumes `merged` and `demo_df` contain \"Municipality_Lowercase\".\n",
    "    \"\"\"\n",
    "    # broadcast demographics\n",
    "    demo_df = broadcast(demo_df)\n",
    "\n",
    "    # calculate year diff\n",
    "    joined = merged.join(demo_df, on=\"Municipality_Lowercase\", how=\"left\") \\\n",
    "                   .withColumn(\"year_diff\", F.abs(F.col(\"Year\") - F.col(\"orig_year\")))\n",
    "\n",
    "    # use window func instead to find closest year\n",
    "    window_spec = Window.partitionBy(\"id\").orderBy(F.col(\"year_diff\").asc(), F.col(\"orig_year\").desc())\n",
    "\n",
    "    # select closest match per id\n",
    "    closest = joined.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "                    .filter(F.col(\"rank\") == 1) \\\n",
    "                    .select(\"id\", ratio_col)\n",
    "    return closest\n",
    "\n",
    "# before joining, we ensure no encoding issues will arise\n",
    "unidecode_udf = udf(lambda x: unidecode(x) if x else x, StringType())\n",
    "\n",
    "merged_df = (\n",
    "    merged_df\n",
    "    .withColumn(\"Municipality_Lowercase\", unidecode_udf(F.col(\"Municipality_Lowercase\")))\n",
    "    .withColumn(\"Municipality_Name\", unidecode_udf(F.col(\"Municipality_Name\")))\n",
    ")\n",
    "df_nation = df_nation.withColumn(\"Municipality_Lowercase\", unidecode_udf(F.col(\"Municipality_Lowercase\")))\n",
    "df_edu = df_edu.withColumn(\"Municipality_Lowercase\", unidecode_udf(F.col(\"Municipality_Lowercase\")))\n",
    "df_age = df_age.withColumn(\"Municipality_Lowercase\", unidecode_udf(F.col(\"Municipality_Lowercase\")))\n",
    "\n",
    "# apply\n",
    "nation_closest = closest_demographic_join(merged_df, df_nation, \"Austria_Ratio\")\n",
    "edu_closest = closest_demographic_join(merged_df, df_edu, \"Uni_Grad_Ratio\")\n",
    "age_closest = closest_demographic_join(merged_df, df_age, \"Pop_65plus_Ratio\")\n",
    "\n",
    "print(\"Sample of nation_closest join results:\")\n",
    "nation_closest.show(3, truncate=False)\n",
    "print(\"Sample of edu_closest join results:\")\n",
    "edu_closest.show(3, truncate=False)\n",
    "print(\"Sample of age_closest join results:\")\n",
    "age_closest.show(3, truncate=False)\n",
    "\n",
    "# join\n",
    "merged_df = (\n",
    "    merged_df\n",
    "    .join(nation_closest, on=\"id\", how=\"left\")\n",
    "    .join(edu_closest, on=\"id\", how=\"left\")\n",
    "    .join(age_closest, on=\"id\", how=\"left\")\n",
    ")\n",
    "print(f\"Row count after joining demographics: {merged_df.count()}\")\n",
    "\n",
    "print(\"Sample merged_df after joining demographics:\")\n",
    "merged_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we will clean and prepare the data for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after cleaning: 1051\n",
      "Spark: DataFrame Schema:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- Municipality_Lowercase: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Municipality_ID: integer (nullable = true)\n",
      " |-- Municipality_Name: string (nullable = true)\n",
      " |-- Wahlberechtigte: integer (nullable = true)\n",
      " |-- abgegebene_Stimmen: integer (nullable = true)\n",
      " |-- gueltige_Stimmen: integer (nullable = true)\n",
      " |-- Wahlbeteiligung: float (nullable = true)\n",
      " |-- Winning_Party: string (nullable = true)\n",
      " |-- Spending_Summe: double (nullable = true)\n",
      " |-- Education_Spending_PC: float (nullable = true)\n",
      " |-- Total_Spending_PC: double (nullable = true)\n",
      " |-- Edu_Spending_Percentage: float (nullable = true)\n",
      " |-- Austria_Ratio: float (nullable = true)\n",
      " |-- Uni_Grad_Ratio: float (nullable = true)\n",
      " |-- Pop_65plus_Ratio: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handle inf/-inf before filtering\n",
    "merged_df = merged_df.replace(float(\"inf\"), None).replace(float(\"-inf\"), None)\n",
    "\n",
    "# Drop rows with missing key columns\n",
    "cols_required = [\"Education_Spending_PC\", \"Edu_Spending_Percentage\", \"Wahlbeteiligung\", \"Winning_Party\",\n",
    "                 \"Austria_Ratio\", \"Uni_Grad_Ratio\", \"Pop_65plus_Ratio\"]\n",
    "merged_df = merged_df.dropna(subset=cols_required)\n",
    "\n",
    "\n",
    "# ensure correct types\n",
    "merged_df = merged_df.withColumn(\"Education_Spending_PC\", F.col(\"Education_Spending_PC\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Edu_Spending_Percentage\", F.col(\"Edu_Spending_Percentage\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Wahlbeteiligung\", F.col(\"Wahlbeteiligung\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Austria_Ratio\", F.col(\"Austria_Ratio\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Uni_Grad_Ratio\", F.col(\"Uni_Grad_Ratio\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Pop_65plus_Ratio\", F.col(\"Pop_65plus_Ratio\").cast(\"float\"))\n",
    "\n",
    "print(f\"Number of rows after cleaning: {merged_df.count()}\")\n",
    "print(\"Spark: DataFrame Schema:\")\n",
    "merged_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will build the model pipeline. First, we get the winning partys. These are the partys that won in the considered elections. Therefore only these will be used for predictions. \n",
    "\n",
    "It is worth noting that while in a closed/stable party systems (like the US), this is fine, as realistically, the number and name of the parties stays roughly consistent. In more open/fluid party systems, like many european ones, where parties frequently emerge/disappear/rebrand, this might yield inaccurate results.\n",
    "\n",
    "However, as accurately predicting such events (e.g. party dissolution, party founding, previously unpopular parties gaining sudden popularity) is incredibly complex (or even impossible) and requires way more social/cultural/political/historical data, this is out of scope for this project.\n",
    "\n",
    "Therefore we assume such events do not occur and the currently popular parties (for our current data ÖVP, SPÖ, FPÖ) continue to dominate in the predicted future election."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels = merged_df.select(\"Winning_Party\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# convert party names into numeric labels.\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"Winning_Party\",\n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"skip\"\n",
    ").setStringOrderType(\"alphabetDesc\")\n",
    "\n",
    "# we assemble a single feature vector from selected columns\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Education_Spending_PC\",\n",
    "        \"Edu_Spending_Percentage\",\n",
    "        \"Wahlbeteiligung\",\n",
    "        \"Austria_Ratio\",\n",
    "        \"Uni_Grad_Ratio\",\n",
    "        \"Pop_65plus_Ratio\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# we train a random forest classifier with 50 trees and a max depth of 5.\n",
    "# if we get out of memory error, we can safely decrease the number of trees.\n",
    "# we will assume the memory can handle 50 trees.\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=50,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# we create the spark pipeline for ML training\n",
    "pipeline = Pipeline(stages=[label_indexer, assembler, rf_classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we train the model and evaluate it. We split the data, predict the future election results, then evaluate the model accuracy, then extract the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|Winning_Party|Label|\n",
      "+-------------+-----+\n",
      "|SPO          |0    |\n",
      "|OVP          |1    |\n",
      "|FPO          |2    |\n",
      "+-------------+-----+\n",
      "\n",
      "+-----------------------+----+-------------+----------+\n",
      "|Municipality_Name      |Year|Winning_Party|prediction|\n",
      "+-----------------------+----+-------------+----------+\n",
      "|Bachmanning            |2019|OVP          |1.0       |\n",
      "|Edt bei Lambach        |2008|SPO          |1.0       |\n",
      "|Dietach                |2019|OVP          |1.0       |\n",
      "|Schenkenfelden         |2008|OVP          |1.0       |\n",
      "|Eggendorf im Traunkreis|2013|OVP          |1.0       |\n",
      "+-----------------------+----+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Test Accuracy = 72.46%\n",
      "\n",
      "Education_Spending_PC: 0.0915\n",
      "Edu_Spending_Percentage: 0.1015\n",
      "Wahlbeteiligung: 0.1927\n",
      "Austria_Ratio: 0.3468\n",
      "Uni_Grad_Ratio: 0.1153\n",
      "Pop_65plus_Ratio: 0.1523\n"
     ]
    }
   ],
   "source": [
    "# split 0.8/0.2\n",
    "train_df, test_df = merged_df.randomSplit([0.8, 0.2], seed=1)\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# for debug, display winning party - label index pairs.\n",
    "party_label_list = [(party, str(index)) for index, party in enumerate(model.stages[0].labels)] # model.stages[0].labels = label names\n",
    "party_label_df = spark.createDataFrame(party_label_list, [\"Winning_Party\", \"Label\"])\n",
    "party_label_df.show(truncate=False)\n",
    "\n",
    "# get predictions on the test set\n",
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"Municipality_Name\", \"Year\", \"Winning_Party\", \"prediction\") \\\n",
    "           .show(5, truncate=False)\n",
    "\n",
    "# evaluate classification accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "print(f\"\\nTest Accuracy = {evaluator.evaluate(predictions) * 100:.2f}%\\n\")\n",
    "\n",
    "# get the feature importances\n",
    "importances = model.stages[-1].featureImportances\n",
    "feature_cols = assembler.getInputCols()\n",
    "\n",
    "# print out the \"importances\" in a readable format. (for raw data: print(importances))\n",
    "for idx, feature_name in enumerate(feature_cols):\n",
    "    print(f\"{feature_name}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "Our model predicted with ~72% accuracy the winning party. We consider this a good accuracy, considering we have only little data on previous elections.\n",
    "\n",
    "As for feature importances, we see that `Austria_Ratio`, i.e. the ratio of austrians to total municipal population, is the strongest predictor with 34.7%. This might indicate that whether areas have a high or low proportion of Austrian citizens significantly influences election outcomes. This could be due to differences in political preferences between predominantly Austrian municipalities and those with larger foreign populations, as well as a reaction to the increase (or decrease) in the share of foreign population in the municipality.\n",
    "\n",
    "After that, `Wahlbeteiligung`, i.e. voter turnout, is the second most powerful feature with 19.3%, indicating that participation levels strongly impact electoral results. Some parties may benefit from higher engagement, while others may perform better when voter turnout is lower. This suggests mobilization efforts or even voter apathy play a key role in election outcomes.\n",
    "\n",
    "Interestingly, `Education_Spending_PC` and `Edu_Spending_Percentage` are the weakest predictors of all considered in this model. This might indicate that education spending is not as strong of a predictor, as voter turnout or other demographic statistics, such as the ratio of Austrians (34.7%) or elderly population (15.2%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will analyze how accurately the model can predict specific parties' winnings and the feature importances for each of the specific partys. For this, we will use the One-vs.-rest classification. While this could be done manually as well (by using a binary label whether winning party = specified party or not), we will use spark's built in OneVsRest method, since it's parallelized, optimized, and scalable.\n",
    "\n",
    "Docs: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.OneVsRest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-Vs-Rest Test Accuracy = 76.44%\n",
      "\n",
      "\n",
      "=== One-vs-Rest for: OVP vs. ALL ===\n",
      "Area Under ROC for OVP vs. All: 0.782\n",
      "  Education_Spending_PC: 0.0787\n",
      "  Edu_Spending_Percentage: 0.0919\n",
      "  Wahlbeteiligung: 0.1666\n",
      "  Austria_Ratio: 0.4238\n",
      "  Uni_Grad_Ratio: 0.1316\n",
      "  Pop_65plus_Ratio: 0.1074\n",
      "\n",
      "=== One-vs-Rest for: SPO vs. ALL ===\n",
      "Area Under ROC for SPO vs. All: 0.788\n",
      "  Education_Spending_PC: 0.1135\n",
      "  Edu_Spending_Percentage: 0.1018\n",
      "  Wahlbeteiligung: 0.2087\n",
      "  Austria_Ratio: 0.3238\n",
      "  Uni_Grad_Ratio: 0.1137\n",
      "  Pop_65plus_Ratio: 0.1386\n",
      "\n",
      "=== One-vs-Rest for: FPO vs. ALL ===\n",
      "Area Under ROC for FPO vs. All: 0.669\n",
      "  Education_Spending_PC: 0.1318\n",
      "  Edu_Spending_Percentage: 0.1448\n",
      "  Wahlbeteiligung: 0.1601\n",
      "  Austria_Ratio: 0.2443\n",
      "  Uni_Grad_Ratio: 0.1739\n",
      "  Pop_65plus_Ratio: 0.1452\n",
      "\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# index the labels into numeric labels\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"Winning_Party\",\n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "label_indexer_model = label_indexer.fit(merged_df) # fit on existing data\n",
    "merged_df = label_indexer_model.transform(merged_df) # apply the mapping to the dataset\n",
    "parties = label_indexer_model.labels  # store the winning party labels\n",
    "\n",
    "# assemble feature vector\n",
    "assembler_ovr = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_ovr\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "merged_df = assembler_ovr.transform(merged_df) # transform to include feature vector\n",
    "\n",
    "# define random forest classifier\n",
    "# this is the base classifier for the OvR. All one-vs-rest classifiers will use random forest.\n",
    "base_classifier = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features_ovr\",\n",
    "    numTrees=30,\n",
    "    maxDepth=5,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# define OVR classifier\n",
    "# it will train a binary classifier (party X or not) for each party.\n",
    "ovr = OneVsRest(classifier=base_classifier, labelCol=\"label\", featuresCol=\"features_ovr\")\n",
    "\n",
    "# # split data, fit on OVR, make predictions, test overall accuracy.\n",
    "train_df, test_df = merged_df.randomSplit([0.8, 0.2], seed=42)\n",
    "ovr_model = ovr.fit(train_df)\n",
    "predictions = ovr_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print(f\"\\nOne-Vs-Rest Test Accuracy = {evaluator.evaluate(predictions) * 100:.2f}%\\n\")\n",
    "\n",
    "# since pyspark's OneVsRest does not directly provide AUC or feature importances,\n",
    "# we will manually compute them for each binary classifier.\n",
    "\n",
    "# parties from labelindexer\n",
    "for i, party in enumerate(parties):\n",
    "    print(f\"\\n=== One-vs-Rest for: {party} vs. ALL ===\")\n",
    "\n",
    "    # get the corresponding binary model for the party\n",
    "    binary_model = ovr_model.models[i]\n",
    "\n",
    "    # create a binary test set for the current party (1 if current party, 0 if not)\n",
    "    test_binary = test_df.withColumn(\"binary_label\", when(col(\"label\") == i, 1).otherwise(0))\n",
    "\n",
    "    # make predictions for AUC evaluation and then compute AUC (area under ROC curve)\n",
    "    binary_preds = binary_model.transform(test_binary)\n",
    "    evaluator_bin = BinaryClassificationEvaluator(labelCol=\"binary_label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    print(f\"Area Under ROC for {party} vs. All: {evaluator_bin.evaluate(binary_preds):.3f}\")\n",
    "\n",
    "    # extract feature importances\n",
    "    importances = binary_model.featureImportances\n",
    "    for j, feature_name in enumerate(feature_cols):\n",
    "        print(f\"  {feature_name}: {importances[j]:.4f}\")\n",
    "\n",
    "print(\"\\nDone\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "The One-Vs-Rest model achieved an overall test accuracy of 76.44%. The AUC scores vary between parties, indicating that the model predicts some outcomes more reliably than others.\n",
    "\n",
    "SPÖ (AUC = 0.788) and ÖVP (AUC = 0.782) are well-predicted, suggesting that their voting patterns align more closely with the features in the dataset.\n",
    "\n",
    "FPÖ (AUC = 0.669) is weaker. This implies that spending, demographic ratios, and turnout do not fully capture FPÖ voting behavior, likely due to external socio-political factors.\n",
    "\n",
    "Education spending (`Education_Spending_PC` and `Edu_Spending_Percentage`) does not seem to be very important in predicting ÖVP/SPÖ votes, however, surprisingly, they appear to be relatively important for FPÖ predictions (13.18% and 14.48% respectively).\n",
    "\n",
    "**ÖVP vs. ALL (AUC = 0.782)**\n",
    "\n",
    "The most influential feature is Austria_Ratio (42.38%), making it the strongest single predictor in the entire model. This suggests that ÖVP is most favored in areas with a high proportion of Austrian-born citizens, which aligns with their historically conservative, rural, and stability-oriented voter base.\n",
    "\n",
    "Voter turnout (16.66%) is the second most important factor, reinforcing that ÖVP benefits from higher electoral participation.\n",
    "\n",
    "**SPÖ vs. ALL (AUC = 0.788)**\n",
    "\n",
    "Austria_Ratio (32.38%) remains a strong predictor but is lower than for ÖVP, reflecting SPÖ’s broader appeal in urban and mixed-demographic areas.\n",
    "\n",
    "Voter turnout (20.87%) is also an influential feature, suggesting SPÖ’s success is highly dependent on mobilization efforts—consistent with center-left parties relying on strong engagement from younger, working-class, and urban voters.\n",
    "\n",
    "**FPÖ vs. ALL (AUC = 0.669)**\n",
    "\n",
    "Austria_Ratio (24.43%) is still a key factor, but it is much weaker than for ÖVP (42.38%), confirming that FPÖ does not rely as exclusively on Austrian-born majority areas. This supports the idea that FPÖ can perform well in both high and low Austria_Ratio areas, depending on economic conditions and voter dissatisfaction.\n",
    "\n",
    "Turnout (16.01%) is important, suggesting FPÖ’s performance is influenced by voter disengagement likely due to its ability to attract protest votes in low-turnout elections.\n",
    "\n",
    "University education ratio (Uni_Grad_Ratio = 17.39%) is surprisingly high, higher than both ÖVP (13.16%) and SPÖ (11.37%). This could indicat:\n",
    "- a shift in FPÖ support among educated voters.\n",
    "- that university education correlates with regions where FPÖ has electoral strength\n",
    "    - note: this might not mean direct FPÖ support from graduates. Peri-urban areas surrounding towns often have a higher percentage of university graduates (compared to rural areas) and a stronger FPÖ presence.\n",
    "\n",
    "The relatively weak AUC suggests FPÖ’s voting patterns are influenced by external socio-political factors not fully captured by spending or demographic data, such as nationalism, anti-immigration sentiment, or dissatisfaction with mainstream politics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `predict_winner_subcategories.ipynb`, we will analyze whether the subcategories of education spending gives us more insight into the prediction of the winning party.\n",
    "\n",
    "In `spending_turnout.ipynb`, we will analyze the connection between spending and voter turnout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

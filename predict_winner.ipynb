{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, we get extra demographics data to improve out predictions and ensure we accurate get the feature importance of spending_data. Obviously, this isn't a foolproof method, but it is better than only using the spending data. New demographics data can be added by modifying the script.\n",
    "\n",
    "Currently we will use demographical data on nationality, gender, age, and highest completed education."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: data/OOE_Bev_Staatsangehoerigkeit.csv\n",
      "Downloaded: data/OOE_Bev_laut_Volkszaehlung_Geschl_Alt5J.csv\n",
      "Downloaded: data/OOE_Bev_Hoechste_abgeschl_Ausbildung.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "\"\"\"\n",
    "URLs for the data.gv.at pages:\n",
    "Date accessed: 2/10/2025 5:41PM\n",
    "\n",
    "Bevölkerung - Staatsangehörigkeit\n",
    "https://www.data.gv.at/katalog/dataset/2a654cd4-60e2-4dc9-ba8d-f7d48343ecf6\n",
    "Bevölkerung - Geschlecht und Alter laut Volkszählung\n",
    "https://www.data.gv.at/katalog/dataset/076e497b-e2e2-409f-a32c-009e05c5f957\n",
    "Bevölkerung - Höchste abgeschlossene Ausbildung\n",
    "https://www.data.gv.at/katalog/dataset/b80ecd53-cfe8-4c9f-9c47-ee567ce94f45\n",
    "\n",
    "The download urls can be found on the [URL]#resources tab\n",
    "\"\"\"\n",
    "\n",
    "urls = [\n",
    "    \"https://e-gov.ooe.gv.at/at.gv.ooe.ogd2-citi/api/file/VAQ5Lkz1NttM9P4OQBxqaw/OOE_Bev_Staatsangehoerigkeit.csv\",\n",
    "    \"https://e-gov.ooe.gv.at/at.gv.ooe.ogd2-citi/api/file/tT_GXC5wGdPvjNfN3GD1ow/OOE_Bev_laut_Volkszaehlung_Geschl_Alt5J.csv\",\n",
    "    \"https://e-gov.ooe.gv.at/at.gv.ooe.ogd2-citi/api/file/E6urCdCJP4L-ZRbWAhzkiA/OOE_Bev_Hoechste_abgeschl_Ausbildung.csv\"\n",
    "]\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "for url in urls:\n",
    "    try:\n",
    "        filename = os.path.join(\"data\", url.split(\"/\")[-1])\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f\"Downloaded: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a scalable pipeline for processing election and demographic data using Apache Spark. We will use Spark DataFrames to handle large datasets efficiently and train a Random Forest classifier to predict the winning party in elections.\n",
    "\n",
    "At first, we initialize spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import broadcast, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from unidecode import unidecode # https://pypi.org/project/Unidecode/\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MunicipalSpendingAndElectionAnalysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the existing merged data that we created in `etl.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data/merged_data.csv\n",
      "First 5 rows now after adding id (and casting Year):\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+-------------------+---+\n",
      "|Municipality_Lowercase|Year|Municipality_ID|Municipality_Name|Wahlberechtigte|abgegebene_Stimmen|gueltige_Stimmen|Wahlbeteiligung|Winning_Party|Spending_Summe     |id |\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+-------------------+---+\n",
      "|linz                  |2008|40101          |Linz             |142125         |96209             |94496           |67.69          |SPO          |1.203534975E8      |0  |\n",
      "|steyr                 |2008|40201          |Steyr            |28962          |20765             |20335           |71.7           |SPO          |3.377888836328125E7|1  |\n",
      "|wels                  |2008|40301          |Wels             |40994          |28803             |28288           |70.26          |SPO          |5.50799846875E7    |2  |\n",
      "|aspach                |2008|40402          |Aspach           |1869           |1390              |1346            |74.37          |OVP          |1433794.4849853516 |3  |\n",
      "|auerbach              |2008|40403          |Auerbach         |411            |310               |301             |75.43          |OVP          |160001.2024230957  |4  |\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+-------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data_path = \"data/merged_data.csv\"\n",
    "merged_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .option(\"sep\", \",\")\n",
    "         .option(\"enforceSchema\", True)\n",
    "         .csv(merged_data_path)\n",
    "         .drop(\"_c0\")  # Remove unnecessary `_c0` index col if appears\n",
    ")\n",
    "\n",
    "print(f\"Loaded {merged_data_path}\")\n",
    "\n",
    "# To join later on demographic data we add a unique ID.\n",
    "merged_df = merged_df.withColumn(\"id\", F.monotonically_increasing_id()) \\\n",
    "                     .withColumn(\"Year\", F.col(\"Year\").cast(\"integer\")) # ensure yr is int\n",
    "\n",
    "print(\"First 5 rows now after adding id (and casting Year):\")\n",
    "merged_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load our demographics data.\n",
    "\n",
    "We calculate the ratio of Austrians per total population, the ratio of university graduates per total pop., and the ratio of 65+ population amongst the total population. If we want to use other ratios, we can adjust the respective part of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample rows from Nationality data:\n",
      "+----------------------+---------+------------------+\n",
      "|Municipality_Lowercase|orig_year|Austria_Ratio     |\n",
      "+----------------------+---------+------------------+\n",
      "|linz                  |2023     |0.7072245498811006|\n",
      "|steyr                 |2023     |0.7656833359625598|\n",
      "|wels                  |2023     |0.6647265152327416|\n",
      "+----------------------+---------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Sample rows from Education Level data:\n",
      "+----------------------+---------+-------------------+\n",
      "|Municipality_Lowercase|orig_year|Uni_Grad_Ratio     |\n",
      "+----------------------+---------+-------------------+\n",
      "|linz                  |2022     |0.15113934789550695|\n",
      "|steyr                 |2022     |0.08382775624291328|\n",
      "|wels                  |2022     |0.08342151126602763|\n",
      "+----------------------+---------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Sample rows from Age data:\n",
      "+----------------------------+---------+-------------------+\n",
      "|Municipality_Lowercase      |orig_year|Pop_65plus_Ratio   |\n",
      "+----------------------------+---------+-------------------+\n",
      "|st.-georgen-am-fillmannsbach|2011     |0.14824120603015076|\n",
      "|diersbach                   |2011     |0.161090458488228  |\n",
      "|stadl-paura                 |2001     |0.15639128647759967|\n",
      "+----------------------------+---------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "def add_muni_col(df, source_col):\n",
    "    \"\"\"\n",
    "    helper function to create a \"Municipality_Lowercase\" col from a source col\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        \"Municipality_Lowercase\",\n",
    "        F.regexp_replace(F.lower(F.col(source_col)), \" \", \"-\")\n",
    "    )\n",
    "\n",
    "# --- A) NATIONALITY (Staatsangehoerigkeit) ---\n",
    "nation_path = \"data/OOE_Bev_Staatsangehoerigkeit.csv\"\n",
    "df_nation = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"sep\", \";\")\n",
    "         .option(\"encoding\", \"latin1\")\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(nation_path)\n",
    ")\n",
    "df_nation = add_muni_col(df_nation, \"LAU2_NAME\") # using helper. LAU2 = municipality name\n",
    "# rename cols for clarity\n",
    "df_nation = df_nation.withColumnRenamed(\"YEAR\", \"orig_year\") \\\n",
    "                     .withColumnRenamed(\"NATION_AUSTRIA\", \"Nation_Austria\") \\\n",
    "                     .withColumnRenamed(\"NATION_TOTAL\", \"Nation_Total\")\n",
    "# cast to numeric (float) as needed\n",
    "df_nation = df_nation.withColumn(\"Nation_Austria\", F.col(\"Nation_Austria\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Nation_Total\", F.col(\"Nation_Total\").cast(\"float\")) \\\n",
    "                     .withColumn(\"orig_year\", F.col(\"orig_year\").cast(\"integer\"))\n",
    "\n",
    "# calculate ratio of austrians per population per municipality\n",
    "df_nation = df_nation.withColumn(\"Austria_Ratio\", F.col(\"Nation_Austria\") / F.col(\"Nation_Total\"))\n",
    "df_nation = df_nation.select(\"Municipality_Lowercase\", \"orig_year\", \"Austria_Ratio\")\n",
    "\n",
    "print(\"Sample rows from Nationality data:\")\n",
    "df_nation.show(3, truncate=False)\n",
    "\n",
    "# --- B) EDUCATION LEVEL (Höchste_abgeschl_Ausbildung) ---\n",
    "edu_path = \"data/OOE_Bev_Hoechste_abgeschl_Ausbildung.csv\"\n",
    "df_edu = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"sep\", \";\")\n",
    "         .option(\"encoding\", \"latin1\")\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(edu_path)\n",
    ")\n",
    "df_edu = add_muni_col(df_edu, \"COMMUNE_NAME\") # using helper\n",
    "# rename cols for clarity\n",
    "df_edu = df_edu.withColumnRenamed(\"YEAR\", \"orig_year\") \\\n",
    "               .withColumnRenamed(\"EDU_UNIVERSITY_FACHHOCHSCHULE\", \"Uni_Grads\") \\\n",
    "               .withColumnRenamed(\"EDU_TOTAL\", \"Edu_Total\")\n",
    "# cast to numeric (float) as needed\n",
    "df_edu = df_edu.withColumn(\"Uni_Grads\", F.col(\"Uni_Grads\").cast(\"float\")) \\\n",
    "               .withColumn(\"Edu_Total\", F.col(\"Edu_Total\").cast(\"float\")) \\\n",
    "               .withColumn(\"orig_year\", F.col(\"orig_year\").cast(\"integer\"))\n",
    "\n",
    "# calculate ratio of uni grads per population per municipality\n",
    "df_edu = df_edu.withColumn(\"Uni_Grad_Ratio\", F.col(\"Uni_Grads\") / F.col(\"Edu_Total\"))\n",
    "df_edu = df_edu.select(\"Municipality_Lowercase\", \"orig_year\", \"Uni_Grad_Ratio\")\n",
    "\n",
    "print(\"Sample rows from Education Level data:\")\n",
    "df_edu.show(3, truncate=False)\n",
    "\n",
    "# --- C) AGE / POPULATION (Bevölkerung laut Volkszählung) ---\n",
    "age_path = \"data/OOE_Bev_laut_Volkszaehlung_Geschl_Alt5J.csv\"\n",
    "df_age = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"sep\", \";\")\n",
    "         .option(\"encoding\", \"latin1\")\n",
    "         .option(\"inferSchema\", True)\n",
    "         .csv(age_path)\n",
    ")\n",
    "df_age = add_muni_col(df_age, \"LAU2_NAME\") # using helper\n",
    "\n",
    "# For our analysis, we will calculate the ratio of 65+ / total population.\n",
    "# If we want to do some other ratio, adjust this code.\n",
    "\n",
    "# columns for 65+\n",
    "age_cols_65plus = [\n",
    "    \"AGE_65_TO_69\", \"AGE_70_TO_74\", \"AGE_75_TO_79\",\n",
    "    \"AGE_80_TO_84\", \"AGE_85_TO_89\", \"AGE_90_PLUS\"\n",
    "]\n",
    "\n",
    "# ensure correct types\n",
    "for c in age_cols_65plus:\n",
    "    df_age = df_age.withColumn(c, F.when(F.col(c).isNull(), 0)\n",
    "                                       .otherwise(F.col(c).cast(\"float\")))\n",
    "df_age = df_age.withColumn(\"AGE_TOTAL\", F.when(F.col(\"AGE_TOTAL\").isNull(), 0)\n",
    "                                         .otherwise(F.col(\"AGE_TOTAL\").cast(\"float\")))\n",
    "# create row-wise sum of the 65+ population\n",
    "df_age = df_age.withColumn(\"POP_65plus\", sum([F.col(c) for c in age_cols_65plus]))\n",
    "df_age = df_age.withColumnRenamed(\"YEAR\", \"orig_year\") \\\n",
    "               .withColumn(\"orig_year\", F.col(\"orig_year\").cast(\"integer\"))\n",
    "\n",
    "# group by municipality and orig_year\n",
    "df_age = df_age.groupBy(\"Municipality_Lowercase\", \"orig_year\") \\\n",
    "               .agg(\n",
    "                   F.sum(\"POP_65plus\").alias(\"POP_65plus\"),\n",
    "                   F.sum(\"AGE_TOTAL\").alias(\"AGE_TOTAL\")\n",
    "               )\n",
    "df_age = df_age.withColumn(\"Pop_65plus_Ratio\", F.col(\"POP_65plus\") / F.col(\"AGE_TOTAL\"))\n",
    "df_age = df_age.select(\"Municipality_Lowercase\", \"orig_year\", \"Pop_65plus_Ratio\")\n",
    "\n",
    "print(\"Sample rows from Age data:\")\n",
    "df_age.show(3, truncate=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1240, 11)\n",
      "(4818, 3)\n",
      "(3066, 3)\n",
      "(2628, 3)\n"
     ]
    }
   ],
   "source": [
    "# dimensions of dataframes\n",
    "print((merged_df.count(), len(merged_df.columns)))\n",
    "print((df_nation.count(), len(df_nation.columns)))\n",
    "print((df_edu.count(), len(df_edu.columns)))\n",
    "print((df_age.count(), len(df_age.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, since demographic data isn't available for every election year, we match each row with closest (in terms of date) available demographic data.\n",
    "\n",
    "While this isn't a perfect solution, it is good enough for our purposes. There are more accurate solutions (e.g. extrapolating data), but for the sake of scalability, and for avoiding unnecessary complexity, we will not do such methods. The time complexity from the initial join is roughly O(N), where N is the number of rows in the main merged dataset.\n",
    "\n",
    "This efficiency is achieved by broadcasting the relatively small demographic datasets, which we will use to join the closest nationality, education, and age ratios to the election data. We will assume the nation, edu, and age dataset fits into memory, so we will use a broadcast join. Thus, the join operation essentially scales linearly with N. The subsequent window function (which orders demographic records per unique id) contributes only a constant factor, keeping the overall complexity at O(N). If the demographic data does not fit into memory, the code can be modified to use a standard left join (O(N log N) due to sorting) or an alternative method like a hash join (O(N) when feasible).\n",
    "\n",
    "Examples of methods if demographic data does not fit into memory:\n",
    "```\n",
    "# left join\n",
    "joined = merged.join(demo_df, on=\"Municipality_Lowercase\", how=\"left\")\n",
    "\n",
    "# disable auto-broadcasting and perform a hash join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "joined = merged.join(demo_df.hint(\"shuffle_hash\"), on=\"Municipality_Lowercase\", how=\"left\")\n",
    "```\n",
    "Broadcast join docs: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.broadcast.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of nation_closest join results:\n",
      "+---+------------------+\n",
      "|id |Austria_Ratio     |\n",
      "+---+------------------+\n",
      "|26 |0.9342235410484668|\n",
      "|29 |0.9445953286257469|\n",
      "|474|0.9523026315789473|\n",
      "+---+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Sample of edu_closest join results:\n",
      "+---+--------------------+\n",
      "|id |Uni_Grad_Ratio      |\n",
      "+---+--------------------+\n",
      "|26 |0.025025025025025027|\n",
      "|29 |0.05830849478390462 |\n",
      "|474|0.03536977491961415 |\n",
      "+---+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Sample of age_closest join results:\n",
      "+---+-------------------+\n",
      "|id |Pop_65plus_Ratio   |\n",
      "+---+-------------------+\n",
      "|26 |0.15015015015015015|\n",
      "|29 |0.17343517138599107|\n",
      "|474|0.19614147909967847|\n",
      "+---+-------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "Row count after joining demographics: 1240\n",
      "Sample merged_df after joining demographics:\n",
      "+---+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+------------------+------------------+--------------------+-------------------+\n",
      "|id |Municipality_Lowercase|Year|Municipality_ID|Municipality_Name|Wahlberechtigte|abgegebene_Stimmen|gueltige_Stimmen|Wahlbeteiligung|Winning_Party|Spending_Summe    |Austria_Ratio     |Uni_Grad_Ratio      |Pop_65plus_Ratio   |\n",
      "+---+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+------------------+------------------+--------------------+-------------------+\n",
      "|26 |tarsdorf              |2008|40443          |Tarsdorf         |1578           |1062              |1010            |67.3           |OVP          |1241949.890625    |0.9342235410484668|0.025025025025025027|0.15015015015015015|\n",
      "|29 |alkoven               |2008|40501          |Alkoven          |4224           |3360              |3276            |79.55          |SPO          |1852952.8623046875|0.9445953286257469|0.05830849478390462 |0.17343517138599107|\n",
      "|474|weilbach              |2013|41235          |Weilbach         |473            |360               |353             |76.11          |OVP          |444505.93994140625|0.9523026315789473|0.03536977491961415 |0.19614147909967847|\n",
      "|964|pupping               |2019|40509          |Pupping          |1455           |981               |971             |67.42          |OVP          |847619.1716403961 |0.9467190128996074|0.06543075245365322 |0.20065430752453653|\n",
      "|65 |grnau-im-almtal       |2008|40707          |Grnau im Almtal  |1665           |1316              |1271            |79.04          |SPO          |639171.8681640625 |null              |null                |null               |\n",
      "+---+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+------------------+------------------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def closest_demographic_join(merged, demo_df, ratio_col):\n",
    "    \"\"\"\n",
    "    Joins demographic data to the main (merged) dataset by selecting the closest available year \n",
    "    for each municipality.\n",
    "\n",
    "    The function broadcasts the demographic dataset to optimize join performance.\n",
    "    Performs a left join on `Municipality_Lowercase` to merge demographic data.\n",
    "    Computes the absolute difference between `Year` (from `merged`) and `orig_year` (from `demo_df`).\n",
    "    Uses a window function to rank rows based on the closest year difference, \n",
    "        with ties resolved by preferring the most recent `orig_year`.\n",
    "    Selects the best match (i.e., the row with rank 1) and returns a DataFrame \n",
    "       containing `id` and the requested demographic ratio column.\n",
    "\n",
    "    :param merged: pyspark.sql.DataFrame election/spendings dataset w/ \n",
    "        The main DataFrame containing the primary dataset with a \"Year\" col.\n",
    "    :param demo_df: pyspark.sql.DataFrame The demographic dataset containing \"Municipality_Lowercase, \"orig_year\", and the ratio col.\n",
    "    ratio_col : str Name of the col in demo_df that contains the demographic ratio to be joined.\n",
    "\n",
    "    :return: pyspark.sql.DataFrame of \"id\" and \"ratio col\"\n",
    "    \n",
    "    Note:\n",
    "    - assumes `merged` and `demo_df` contain \"Municipality_Lowercase\".\n",
    "    \"\"\"\n",
    "    # broadcast demographics\n",
    "    demo_df = broadcast(demo_df)\n",
    "\n",
    "    # calculate year diff\n",
    "    joined = merged.join(demo_df, on=\"Municipality_Lowercase\", how=\"left\") \\\n",
    "                   .withColumn(\"year_diff\", F.abs(F.col(\"Year\") - F.col(\"orig_year\")))\n",
    "\n",
    "    # use window func instead to find closest year\n",
    "    window_spec = Window.partitionBy(\"id\").orderBy(F.col(\"year_diff\").asc(), F.col(\"orig_year\").desc())\n",
    "\n",
    "    # select closest match per id\n",
    "    closest = joined.withColumn(\"rank\", F.row_number().over(window_spec)) \\\n",
    "                    .filter(F.col(\"rank\") == 1) \\\n",
    "                    .select(\"id\", ratio_col)\n",
    "    return closest\n",
    "\n",
    "# before joining, we ensure no encoding issues will arise\n",
    "unidecode_udf = udf(lambda x: unidecode(x) if x else x, StringType())\n",
    "\n",
    "merged_df = (\n",
    "    merged_df\n",
    "    .withColumn(\"Municipality_Lowercase\", unidecode_udf(F.col(\"Municipality_Lowercase\")))\n",
    "    .withColumn(\"Municipality_Name\", unidecode_udf(F.col(\"Municipality_Name\")))\n",
    ")\n",
    "df_nation = df_nation.withColumn(\"Municipality_Lowercase\", unidecode_udf(F.col(\"Municipality_Lowercase\")))\n",
    "df_edu = df_edu.withColumn(\"Municipality_Lowercase\", unidecode_udf(F.col(\"Municipality_Lowercase\")))\n",
    "df_age = df_age.withColumn(\"Municipality_Lowercase\", unidecode_udf(F.col(\"Municipality_Lowercase\")))\n",
    "\n",
    "# apply\n",
    "nation_closest = closest_demographic_join(merged_df, df_nation, \"Austria_Ratio\")\n",
    "edu_closest = closest_demographic_join(merged_df, df_edu, \"Uni_Grad_Ratio\")\n",
    "age_closest = closest_demographic_join(merged_df, df_age, \"Pop_65plus_Ratio\")\n",
    "\n",
    "print(\"Sample of nation_closest join results:\")\n",
    "nation_closest.show(3, truncate=False)\n",
    "print(\"Sample of edu_closest join results:\")\n",
    "edu_closest.show(3, truncate=False)\n",
    "print(\"Sample of age_closest join results:\")\n",
    "age_closest.show(3, truncate=False)\n",
    "\n",
    "# join\n",
    "merged_df = (\n",
    "    merged_df\n",
    "    .join(nation_closest, on=\"id\", how=\"left\")\n",
    "    .join(edu_closest, on=\"id\", how=\"left\")\n",
    "    .join(age_closest, on=\"id\", how=\"left\")\n",
    ")\n",
    "print(f\"Row count after joining demographics: {merged_df.count()}\")\n",
    "\n",
    "print(\"Sample merged_df after joining demographics:\")\n",
    "merged_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we will clean and prepare the data for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after cleaning: 1051\n",
      "Spark: DataFrame Schema:\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- Municipality_Lowercase: string (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Municipality_ID: integer (nullable = true)\n",
      " |-- Municipality_Name: string (nullable = true)\n",
      " |-- Wahlberechtigte: integer (nullable = true)\n",
      " |-- abgegebene_Stimmen: integer (nullable = true)\n",
      " |-- gueltige_Stimmen: integer (nullable = true)\n",
      " |-- Wahlbeteiligung: float (nullable = true)\n",
      " |-- Winning_Party: string (nullable = true)\n",
      " |-- Spending_Summe: float (nullable = true)\n",
      " |-- Austria_Ratio: float (nullable = true)\n",
      " |-- Uni_Grad_Ratio: float (nullable = true)\n",
      " |-- Pop_65plus_Ratio: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handle inf/-inf before filtering\n",
    "merged_df = merged_df.replace(float(\"inf\"), None).replace(float(\"-inf\"), None)\n",
    "\n",
    "# Drop rows with missing key columns\n",
    "cols_required = [\"Spending_Summe\", \"Wahlbeteiligung\", \"Winning_Party\",\n",
    "                 \"Austria_Ratio\", \"Uni_Grad_Ratio\", \"Pop_65plus_Ratio\"]\n",
    "merged_df = merged_df.dropna(subset=cols_required)\n",
    "\n",
    "\n",
    "# ensure correct types\n",
    "merged_df = merged_df.withColumn(\"Spending_Summe\", F.col(\"Spending_Summe\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Wahlbeteiligung\", F.col(\"Wahlbeteiligung\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Austria_Ratio\", F.col(\"Austria_Ratio\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Uni_Grad_Ratio\", F.col(\"Uni_Grad_Ratio\").cast(\"float\")) \\\n",
    "                     .withColumn(\"Pop_65plus_Ratio\", F.col(\"Pop_65plus_Ratio\").cast(\"float\"))\n",
    "\n",
    "print(f\"Number of rows after cleaning: {merged_df.count()}\")\n",
    "print(\"Spark: DataFrame Schema:\")\n",
    "merged_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will build the model pipeline. First, we get the winning partys. These are the partys that won in the considered elections. Therefore only these will be used for predictions. \n",
    "\n",
    "It is worth noting that while in a closed/stable party systems (like the US), this is fine, as realistically, the number and name of the parties stays roughly consistent. In more open/fluid party systems, like many european ones, where parties frequently emerge/disappear/rebrand, this might yield inaccurate results.\n",
    "\n",
    "However, as accurately predicting such events (e.g. party dissolution, party founding, previously unpopular parties gaining sudden popularity) is incredibly complex (or even impossible) and requires way more social/cultural/political/historical data, this is out of scope for this project.\n",
    "\n",
    "Therefore we assume such events do not occur and the currently popular parties (for our current data ÖVP, SPÖ, FPÖ) continue to dominate in the predicted future election."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "party_labels = merged_df.select(\"Winning_Party\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# convert party names into numeric labels.\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"Winning_Party\",\n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"skip\"\n",
    ").setStringOrderType(\"alphabetDesc\")\n",
    "\n",
    "# we assemble a single feature vector from selected columns\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        \"Spending_Summe\",\n",
    "        \"Wahlbeteiligung\",\n",
    "        \"Austria_Ratio\",\n",
    "        \"Uni_Grad_Ratio\",\n",
    "        \"Pop_65plus_Ratio\",\n",
    "    ],\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "\n",
    "# we train a random forest classifier with 50 trees and a max depth of 5.\n",
    "# if we get out of memory error, we can safely decrease the number of trees.\n",
    "# we will assume the memory can handle 50 trees.\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    numTrees=50,\n",
    "    maxDepth=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# we create the spark pipeline for ML training\n",
    "pipeline = Pipeline(stages=[label_indexer, assembler, rf_classifier])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we train the model and evaluate it. We split the data, predict the future election results, then evaluate the model accuracy, then extract the feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|Winning_Party|Label|\n",
      "+-------------+-----+\n",
      "|SPO          |0    |\n",
      "|OVP          |1    |\n",
      "|FPO          |2    |\n",
      "+-------------+-----+\n",
      "\n",
      "+-----------------------+----+-------------+----------+\n",
      "|Municipality_Name      |Year|Winning_Party|prediction|\n",
      "+-----------------------+----+-------------+----------+\n",
      "|Bachmanning            |2019|OVP          |1.0       |\n",
      "|Edt bei Lambach        |2008|SPO          |1.0       |\n",
      "|Dietach                |2019|OVP          |1.0       |\n",
      "|Schenkenfelden         |2008|OVP          |1.0       |\n",
      "|Eggendorf im Traunkreis|2013|OVP          |1.0       |\n",
      "+-----------------------+----+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Test Accuracy = 75.36%\n",
      "\n",
      "Spending_Summe: 0.3874\n",
      "Wahlbeteiligung: 0.2048\n",
      "Austria_Ratio: 0.1713\n",
      "Uni_Grad_Ratio: 0.1333\n",
      "Pop_65plus_Ratio: 0.1032\n"
     ]
    }
   ],
   "source": [
    "# split 0.8/0.2\n",
    "train_df, test_df = merged_df.randomSplit([0.8, 0.2], seed=1)\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# for debug, display winning party - label index pairs.\n",
    "party_label_list = [(party, str(index)) for index, party in enumerate(model.stages[0].labels)] # model.stages[0].labels = label names\n",
    "party_label_df = spark.createDataFrame(party_label_list, [\"Winning_Party\", \"Label\"])\n",
    "party_label_df.show(truncate=False)\n",
    "\n",
    "# get predictions on the test set\n",
    "predictions = model.transform(test_df)\n",
    "predictions.select(\"Municipality_Name\", \"Year\", \"Winning_Party\", \"prediction\") \\\n",
    "           .show(5, truncate=False)\n",
    "\n",
    "# evaluate classification accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "print(f\"\\nTest Accuracy = {evaluator.evaluate(predictions) * 100:.2f}%\\n\")\n",
    "\n",
    "# get the feature importances\n",
    "importances = model.stages[-1].featureImportances\n",
    "feature_cols = assembler.getInputCols()\n",
    "\n",
    "# print out the \"importances\" in a readable format. (for raw data: print(importances))\n",
    "for idx, feature_name in enumerate(feature_cols):\n",
    "    print(f\"{feature_name}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "Our model predicted with ~75% accuracy the winning party. We consider this a good accuracy, considering we have only little data on previous elections.\n",
    "\n",
    "We can also see that the sum of education spending is the most important feature our of the 5 we considered. This raises further questions on what sub-category of education spending is the most important in predicting election results. We will do this analysis in another script (`predict_winner_subcategories.ipynb`).\n",
    "\n",
    "Additionally, the voter turnout (Wahlbeteiligung) seems also relatively significant. This raises the question how voter turnout influences election results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

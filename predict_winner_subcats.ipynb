{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script, we will continue the winner prediction, but we will use subcategories instead of the sum of education spending. Additionally, we will predict the accuracy for predicting each of the 3 major parties.\n",
    "\n",
    "We will start with getting/initializing a spark session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import json\n",
    "import re\n",
    "from unidecode import unidecode # https://pypi.org/project/Unidecode/\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, when, regexp_replace, udf\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassifier, OneVsRest\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MunicipalSpendingAndElectionAnalysis\") \\\n",
    "    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will load the data. First, we load the election/spending dataset (data/merged_data.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Election data:\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+-------------------+---------------------+-----------------+-----------------------+\n",
      "|Municipality_Lowercase|Year|Municipality_ID|Municipality_Name|Wahlberechtigte|abgegebene_Stimmen|gueltige_Stimmen|Wahlbeteiligung|Winning_Party|Spending_Summe     |Education_Spending_PC|Total_Spending_PC|Edu_Spending_Percentage|\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+-------------------+---------------------+-----------------+-----------------------+\n",
      "|linz                  |2008|40101          |Linz             |142125         |96209             |94496           |67.69          |SPO          |1.203534975E8      |319.61816451293      |3378.0490982967  |9.461619864379408      |\n",
      "|steyr                 |2008|40201          |Steyr            |28962          |20765             |20335           |71.7           |SPO          |3.377888836328125E7|436.81479839648      |3132.9064381223  |13.942797431840445     |\n",
      "|wels                  |2008|40301          |Wels             |40994          |28803             |28288           |70.26          |SPO          |5.50799846875E7    |471.85801422085      |3152.7184429024  |14.966703267877495     |\n",
      "|aspach                |2008|40402          |Aspach           |1869           |1390              |1346            |74.37          |OVP          |1433794.4849853516 |306.75962772786      |2834.8965554129  |10.82084025754445      |\n",
      "|auerbach              |2008|40403          |Auerbach         |411            |310               |301             |75.43          |OVP          |160001.2024230957  |152.3820952381       |2021.9030666667  |7.536567788549646      |\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+-------------------+---------------------+-----------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Subcategory spending:\n",
      "+----------------------+----+------------------------------------+--------------------------------------+-------------------------+-------------------------+---------------------------------+---------------------------------+----------------------------+-------------------------------------------------+------------+------------------------------+\n",
      "|Municipality_Lowercase|Year|SubCat_Allgemeinbildender_Unterricht|SubCat_Ausserschulische_Jugenerziehung|SubCat_Berufsb_Unterricht|SubCat_Erwachsenenbildung|SubCat_Forschung_und_Wissenschaft|SubCat_Forderung_des_Unterrichtes|SubCat_Gesonderte_Verwaltung|SubCat_Sport_und_ausserschulische_Leibeserziehung|SubCat_Summe|SubCat_Vorschulische_Erziehung|\n",
      "+----------------------+----+------------------------------------+--------------------------------------+-------------------------+-------------------------+---------------------------------+---------------------------------+----------------------------+-------------------------------------------------+------------+------------------------------+\n",
      "|adlwang               |2007|585724.1875                         |54450.37890625                        |13776.25                 |200.0                    |0.0                              |12545.3896484375                 |0.0                         |13957.1103515625                                 |945950.875  |265297.59375                  |\n",
      "|waldneukirchen        |2008|293467.84375                        |14717.73046875                        |39020.55859375           |0.0                      |0.0                              |20067.390625                     |0.0                         |38048.171875                                     |503741.65625|98419.953125                  |\n",
      "|gaspoltshofen         |2013|962090.3125                         |1340.5699462890625                    |60795.91015625           |400.0                    |1385.0                           |112647.7421875                   |0.0                         |23207.470703125                                  |1394021.5   |232154.484375                 |\n",
      "|kirchheim-im-innkreis |2014|86279.421875                        |0.0                                   |4443.5                   |0.0                      |0.0                              |454.25                           |0.0                         |1973.52001953125                                 |270297.4375 |177146.734375                 |\n",
      "|eferding              |2014|861382.0                            |504725.65625                          |84063.75                 |39020.53125              |0.0                              |1035.0                           |0.0                         |82906.21875                                      |2586008.25  |1012875.0625                  |\n",
      "+----------------------+----+------------------------------------+--------------------------------------+-------------------------+-------------------------+---------------------------------+---------------------------------+----------------------------+-------------------------------------------------+------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_data_path = \"data/merged_data.csv\"\n",
    "merged_df = (\n",
    "    spark.read\n",
    "         .option(\"header\", True)\n",
    "         .option(\"inferSchema\", True)\n",
    "         .option(\"sep\", \",\")\n",
    "         .csv(merged_data_path)\n",
    "         .drop(\"_c0\")  # Drop unnecessary column if exists\n",
    ")\n",
    "\n",
    "# ensure proper types\n",
    "merged_df = merged_df.withColumn(\"Year\", F.col(\"Year\").cast(\"integer\"))\n",
    "\n",
    "# ensure col name consistency.\n",
    "if \"Municipality\" in merged_df.columns:\n",
    "    merged_df = merged_df.withColumnRenamed(\"Municipality\", \"Municipality_Name\")\n",
    "if \"Municipality_Lowercase\" not in merged_df.columns:\n",
    "    merged_df = merged_df.withColumn(\n",
    "        \"Municipality_Lowercase\",\n",
    "        F.regexp_replace(F.lower(F.trim(col(\"Municipality_Name\"))), r\"\\s+\", \"-\")\n",
    "    )\n",
    "\n",
    "print(\"Election data:\")\n",
    "merged_df.show(5, truncate=False)\n",
    "\n",
    "# we will load the spending data again to get the subcategory-level spending.\n",
    "spend_raw = spark.read.csv(\"data/Bildungsausgaben_Gemeinden_Ober√∂sterreich_data_2007_bis_2019.csv\",\n",
    "                           header=True, sep=\",\", inferSchema=True)\n",
    "\n",
    "# rename cols for clarity\n",
    "spend_raw = spend_raw.withColumnRenamed(\"Gemeinde\", \"Municipality\")\\\n",
    "                     .withColumnRenamed(\"Year\", \"Year\")\\\n",
    "                     .withColumnRenamed(\"Abschnitt\", \"Subcategory\")\\\n",
    "                     .withColumnRenamed(\"Betrag in Euro\", \"Spending\")\n",
    "\n",
    "# Add a lowercase municipality column for joining later\n",
    "spend_raw = spend_raw.withColumn(\n",
    "    \"Municipality_Lowercase\",\n",
    "    F.regexp_replace(F.lower(F.trim(col(\"Municipality\"))), r\"\\s+\", \"-\")\n",
    ")\n",
    "\n",
    "spend_raw = spend_raw.withColumn(\"Year\", col(\"Year\").cast(\"int\"))\\\n",
    "                     .withColumn(\"Spending\", col(\"Spending\").cast(\"float\"))\n",
    "\n",
    "# pivot the spending data so that each subcat. becomes its own col.\n",
    "pivot_spend = spend_raw.groupBy(\"Municipality_Lowercase\", \"Year\") \\\n",
    "                       .pivot(\"Subcategory\") \\\n",
    "                       .agg(F.sum(\"Spending\"))\n",
    "pivot_spend = pivot_spend.na.fill(0)\n",
    "\n",
    "# remove special chars, add SubCat_ prefix.\n",
    "def clean_column_name(col_name):\n",
    "    \"\"\"\n",
    "    remove special characters\n",
    "    replace not alphanumeric characters with underscore\n",
    "    add SubCat_ prefix\n",
    "    \n",
    "    sidenote: \n",
    "        unidecode replaces special chars with similar ASCII chars, so F√∂rderung -> Forderung\n",
    "        while this changes the word meaning, context helps avoiding misunderstandings\n",
    "    \n",
    "    :param col_name: column name for which func should be applied to\n",
    "    :return: new column name\n",
    "    \"\"\"\n",
    "    col_name = unidecode(col_name)  # special char -> ascii\n",
    "    col_name = re.sub(r\"[^a-zA-Z0-9]+\", \"_\", col_name.strip())  # not alphanum -> \"_\"\n",
    "    return f\"SubCat_{col_name}\"  # add SubCat_\n",
    "\n",
    "# apply cleaning to subcategory columns\n",
    "for old_col in pivot_spend.columns:\n",
    "    if old_col not in [\"Municipality_Lowercase\", \"Year\"]:\n",
    "        new_col = clean_column_name(old_col)\n",
    "        pivot_spend = pivot_spend.withColumnRenamed(old_col, new_col)\n",
    "\n",
    "print(\"Subcategory spending:\")\n",
    "pivot_spend.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will merge the election results dataset (merged_df) with the spending dataset (pivot_spend). After that, we will clean and prepare the merged dataset for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged election + spending data:\n",
      "+---------------+\n",
      "|Wahlbeteiligung|\n",
      "+---------------+\n",
      "|82.12          |\n",
      "|68.22          |\n",
      "|67.95          |\n",
      "|67.12          |\n",
      "|62.09          |\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Feature columns used for modeling: ['SubCat_Allgemeinbildender_Unterricht_PC', 'SubCat_Ausserschulische_Jugenerziehung_PC', 'SubCat_Berufsb_Unterricht_PC', 'SubCat_Erwachsenenbildung_PC', 'SubCat_Forschung_und_Wissenschaft_PC', 'SubCat_Forderung_des_Unterrichtes_PC', 'SubCat_Gesonderte_Verwaltung_PC', 'SubCat_Sport_und_ausserschulische_Leibeserziehung_PC', 'SubCat_Vorschulische_Erziehung_PC', 'Wahlbeteiligung', 'Education_Spending_PC', 'Edu_Spending_Percentage']\n",
      "Merged data (post cleaning):\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+------------------+---------------------+-----------------+-----------------------+------------+---------------------------------------+-----------------------------------------+----------------------------+----------------------------+------------------------------------+------------------------------------+-------------------------------+----------------------------------------------------+---------------------------------+\n",
      "|Municipality_Lowercase|Year|Municipality_ID|Municipality_Name|Wahlberechtigte|abgegebene_Stimmen|gueltige_Stimmen|Wahlbeteiligung|Winning_Party|Spending_Summe    |Education_Spending_PC|Total_Spending_PC|Edu_Spending_Percentage|SubCat_Summe|SubCat_Allgemeinbildender_Unterricht_PC|SubCat_Ausserschulische_Jugenerziehung_PC|SubCat_Berufsb_Unterricht_PC|SubCat_Erwachsenenbildung_PC|SubCat_Forschung_und_Wissenschaft_PC|SubCat_Forderung_des_Unterrichtes_PC|SubCat_Gesonderte_Verwaltung_PC|SubCat_Sport_und_ausserschulische_Leibeserziehung_PC|SubCat_Vorschulische_Erziehung_PC|\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+------------------+---------------------+-----------------+-----------------------+------------+---------------------------------------+-----------------------------------------+----------------------------+----------------------------+------------------------------------+------------------------------------+-------------------------------+----------------------------------------------------+---------------------------------+\n",
      "|waldneukirchen        |2008|41518          |Waldneukirchen   |1723           |1415              |1374            |82.12          |OVP          |1007483.3046875   |228.45427            |1698.9230294785  |13.447005              |503741.65625|133.092                                |6.674708                                 |17.696398                   |0.0                         |0.0                                 |9.100858                            |0.0                            |17.255407                                           |44.6349                          |\n",
      "|gaspoltshofen         |2013|40806          |Gaspoltshofen    |2819           |1923              |1856            |68.22          |FPO          |2788042.989868164 |397.95074            |2513.7077248073  |15.831226              |1394021.5   |274.64752                              |0.38269195                               |17.355383                   |0.11418784                  |0.39537537                          |32.157505                           |0.0                            |6.625027                                            |66.27305                         |\n",
      "|pabneukirchen         |2019|41115          |Pabneukirchen    |1329           |903               |883             |67.95          |OVP          |3370664.49609375  |990.207              |3034.3461750881  |32.63329               |1685332.25  |808.4454                               |0.0                                      |8.335435                    |0.23501763                  |0.117508814                         |38.35623                            |0.0                            |20.641083                                           |114.076324                       |\n",
      "|schwanenstadt         |2013|41738          |Schwanenstadt    |2947           |1978              |1933            |67.12          |SPO          |3064395.228515625 |377.5746             |3073.7619935929  |12.283793              |1532197.625 |239.99927                              |34.42406                                 |7.8124843                   |0.2030557                   |0.012321341                         |10.55503                            |0.0                            |41.036594                                           |43.53177                         |\n",
      "|oberwang              |2019|41721          |Oberwang         |1377           |855               |838             |62.09          |OVP          |1202924.3754882812|345.27106            |2197.0773249139  |15.715016              |601462.1875 |120.37285                              |0.0                                      |13.180206                   |0.0                         |0.0                                 |2.8325372                           |0.0                            |7.9886737                                           |200.89679                        |\n",
      "+----------------------+----+---------------+-----------------+---------------+------------------+----------------+---------------+-------------+------------------+---------------------+-----------------+-----------------------+------------+---------------------------------------+-----------------------------------------+----------------------------+----------------------------+------------------------------------+------------------------------------+-------------------------------+----------------------------------------------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_df = merged_df.join(pivot_spend, on=[\"Municipality_Lowercase\", \"Year\"], how=\"inner\")\n",
    "merged_df = merged_df.na.drop(subset=[\"Winning_Party\"]) # if winning party is missing, drop\n",
    "\n",
    "# fill NA with 0 for numeric\n",
    "numeric_cols = [f.name for f in merged_df.schema.fields if f.dataType in [IntegerType(), FloatType()]]\n",
    "for ncol in numeric_cols:\n",
    "    merged_df = merged_df.withColumn(ncol, when(col(ncol).isNull(), 0).otherwise(col(ncol)))\n",
    "\n",
    "print(\"Merged election + spending data:\")\n",
    "merged_df.cache() # cache because we want quick access to merged_df\n",
    "merged_df.select(\"Wahlbeteiligung\").show(5, truncate=False)\n",
    "\n",
    "# now we will clean Wahlbeteiligung col values\n",
    "\n",
    "# For our current datasets, this is mostly redundant\n",
    "#  but we will clean it to make sure the script works with other datasets as well\n",
    "\n",
    "# we use a single nested regexp_replace instead of doing each of these individually\n",
    "#  to make it more efficient and reduce spark overhead\n",
    "merged_df = merged_df.withColumn(\n",
    "    \"Wahlbeteiligung\",\n",
    "    regexp_replace(\n",
    "        regexp_replace(\n",
    "            regexp_replace(col(\"Wahlbeteiligung\"), \"%\", \"\"),  # remove %\n",
    "            \",\", \".\"                                          # replace commas with dots\n",
    "        ),\n",
    "        \"[^\\\\d.]\", \"\"                                        # remove non-numeric chars except dots\n",
    "    ).cast(\"float\")                                          # convert to float\n",
    ")\n",
    "\n",
    "# identify subcategory columns (columns that start with \"SubCat_\")\n",
    "# we exclude SubCat_Summe because it is just the sum of the subcategories and\n",
    "#  including them would thus distort the results\n",
    "# Ensure the new columns are available and cast to float\n",
    "for col_name in [\"Wahlbeteiligung\", \"Education_Spending_PC\", \"Edu_Spending_Percentage\"]:\n",
    "    merged_df = merged_df.withColumn(col_name, F.col(col_name).cast(\"float\"))\n",
    "    \n",
    "subcat_cols = [col for col in merged_df.columns if col.startswith(\"SubCat_\") and col != \"SubCat_Summe\"]\n",
    "\n",
    "# for each subcategory, we compute its per capita value:\n",
    "# SubCat_*_PC = (absolute subcat spending / SubCat_Summe) * Education_Spending_PC\n",
    "for c in subcat_cols:\n",
    "    new_col_name = c + \"_PC\"\n",
    "    merged_df = merged_df.withColumn(\n",
    "        new_col_name,\n",
    "        F.when(F.col(\"SubCat_Summe\") == 0, 0)\n",
    "         .otherwise((F.col(c) / F.col(\"SubCat_Summe\")) * F.col(\"Education_Spending_PC\"))\n",
    "    )\n",
    "# drop absolute subcategory spendings\n",
    "merged_df = merged_df.drop(*subcat_cols)\n",
    "\n",
    "# new per capita subcat cols\n",
    "subcat_pc_cols = [c + \"_PC\" for c in subcat_cols]\n",
    "# add following to feature cols, we will use them in model training\n",
    "feature_cols = subcat_pc_cols  + [\"Wahlbeteiligung\", \"Education_Spending_PC\", \"Edu_Spending_Percentage\"]\n",
    "\n",
    "print(\"Feature columns used for modeling:\", feature_cols)\n",
    "\n",
    "# Ensure that all feature columns are of type float\n",
    "for f in feature_cols:\n",
    "    merged_df = merged_df.withColumn(f, col(f).cast(\"float\"))\n",
    "\n",
    "merged_df.cache()\n",
    "print(\"Merged data (post cleaning):\")\n",
    "merged_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the dataframe, we will build the spark pipeline and train the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert the winning party values into numeric labels (0.0, 1.0 etc.)\n",
    "label_indexer = StringIndexer(inputCol=\"Winning_Party\", outputCol=\"label\", handleInvalid=\"skip\")\n",
    "\n",
    "# we will assemble the feature cols into a single feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\", handleInvalid=\"skip\")\n",
    "\n",
    "# we define the RF classifier and then create the pipeline\n",
    "rf_classifier = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=30, maxDepth=5, seed=1)\n",
    "pipeline = Pipeline(stages=[label_indexer, assembler, rf_classifier])\n",
    "\n",
    "# we split the data, train the model\n",
    "train_df, test_df = merged_df.randomSplit([0.8, 0.2], seed=1)\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# we make predictions on the test_set\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# for debug, display winning party - label index pairs.\n",
    "party_label_list = [(party, str(index)) for index, party in enumerate(model.stages[0].labels)] # model.stages[0].labels = label names\n",
    "party_label_df = spark.createDataFrame(party_label_list, [\"Winning_Party\", \"Label\"])\n",
    "party_label_df.show(truncate=False)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "predictions.select(\"Municipality_Name\", \"Year\", \"Winning_Party\", \"prediction\").show(5, truncate=False)\n",
    "\n",
    "# Now we evaulate model accuracy and get the feature importances.\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print(f\"\\nTest Accuracy = {evaluator.evaluate(predictions) * 100:.2f}%\\n\")\n",
    "\n",
    "importances = model.stages[-1].featureImportances\n",
    "for idx, feature_name in enumerate(assembler.getInputCols()):\n",
    "    print(f\"{feature_name}: {importances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "Our model predicted with ~71.6% accuracy the winning party. We consider this a good accuracy, considering we have only little data on previous elections.\n",
    "\n",
    "Now that we analyzed the feature importance of the subcategories, we can deduct that `Erwachsenenbildung_PC` have the highest importance in predicting the winning party (11.85%). After that, `Wahlbeteiligung` and `SubCat_Gesonderte_Verwaltung_PC` are also relatively important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will analyze how accurately the model can predict specific parties' winnings and the feature importances for each of the specific partys. For this, we will use the One-vs.-rest classification. While this could be done manually as well (by using a binary label whether winning party = specified party or not), we will use spark's built in OneVsRest method, since it's parallelized, optimized, and scalable.\n",
    "\n",
    "Docs: https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.classification.OneVsRest.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "One-Vs-Rest Test Accuracy = 78.04%\n",
      "\n",
      "\n",
      "=== One-vs-Rest for: OVP vs. ALL ===\n",
      "Area Under ROC for OVP vs. All: 0.772\n",
      "  SubCat_Allgemeinbildender_Unterricht_PC: 0.0816\n",
      "  SubCat_Ausserschulische_Jugenerziehung_PC: 0.2043\n",
      "  SubCat_Berufsb_Unterricht_PC: 0.0566\n",
      "  SubCat_Erwachsenenbildung_PC: 0.1429\n",
      "  SubCat_Forschung_und_Wissenschaft_PC: 0.0951\n",
      "  SubCat_Forderung_des_Unterrichtes_PC: 0.0452\n",
      "  SubCat_Gesonderte_Verwaltung_PC: 0.0890\n",
      "  SubCat_Sport_und_ausserschulische_Leibeserziehung_PC: 0.0564\n",
      "  SubCat_Vorschulische_Erziehung_PC: 0.0461\n",
      "  Wahlbeteiligung: 0.0851\n",
      "  Education_Spending_PC: 0.0414\n",
      "  Edu_Spending_Percentage: 0.0563\n",
      "\n",
      "=== One-vs-Rest for: SPO vs. ALL ===\n",
      "Area Under ROC for SPO vs. All: 0.839\n",
      "  SubCat_Allgemeinbildender_Unterricht_PC: 0.0767\n",
      "  SubCat_Ausserschulische_Jugenerziehung_PC: 0.1500\n",
      "  SubCat_Berufsb_Unterricht_PC: 0.0608\n",
      "  SubCat_Erwachsenenbildung_PC: 0.1249\n",
      "  SubCat_Forschung_und_Wissenschaft_PC: 0.1016\n",
      "  SubCat_Forderung_des_Unterrichtes_PC: 0.0349\n",
      "  SubCat_Gesonderte_Verwaltung_PC: 0.1470\n",
      "  SubCat_Sport_und_ausserschulische_Leibeserziehung_PC: 0.0418\n",
      "  SubCat_Vorschulische_Erziehung_PC: 0.0725\n",
      "  Wahlbeteiligung: 0.0810\n",
      "  Education_Spending_PC: 0.0463\n",
      "  Edu_Spending_Percentage: 0.0624\n",
      "\n",
      "=== One-vs-Rest for: FPO vs. ALL ===\n",
      "Area Under ROC for FPO vs. All: 0.633\n",
      "  SubCat_Allgemeinbildender_Unterricht_PC: 0.1363\n",
      "  SubCat_Ausserschulische_Jugenerziehung_PC: 0.0712\n",
      "  SubCat_Berufsb_Unterricht_PC: 0.0992\n",
      "  SubCat_Erwachsenenbildung_PC: 0.0505\n",
      "  SubCat_Forschung_und_Wissenschaft_PC: 0.0380\n",
      "  SubCat_Forderung_des_Unterrichtes_PC: 0.0713\n",
      "  SubCat_Gesonderte_Verwaltung_PC: 0.0350\n",
      "  SubCat_Sport_und_ausserschulische_Leibeserziehung_PC: 0.0983\n",
      "  SubCat_Vorschulische_Erziehung_PC: 0.0807\n",
      "  Wahlbeteiligung: 0.1078\n",
      "  Education_Spending_PC: 0.0797\n",
      "  Edu_Spending_Percentage: 0.1319\n",
      "\n",
      "Done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# index the labels into numeric labels\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"Winning_Party\",\n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "label_indexer_model = label_indexer.fit(merged_df) # fit on existing data\n",
    "merged_df = label_indexer_model.transform(merged_df) # apply the mapping to the dataset\n",
    "parties = label_indexer_model.labels  # store the winning party labels\n",
    "\n",
    "# assemble feature vector\n",
    "assembler_ovr = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features_ovr\",\n",
    "    handleInvalid=\"skip\"\n",
    ")\n",
    "merged_df = assembler_ovr.transform(merged_df) # transform to include feature vector\n",
    "\n",
    "# define random forest classifier\n",
    "# this is the base classifier for the OvR. All one-vs-rest classifiers will use random forest.\n",
    "base_classifier = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features_ovr\",\n",
    "    numTrees=30,\n",
    "    maxDepth=5,\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "# define OVR classifier\n",
    "# it will train a binary classifier (party X or not) for each party.\n",
    "ovr = OneVsRest(classifier=base_classifier, labelCol=\"label\", featuresCol=\"features_ovr\")\n",
    "\n",
    "# # split data, fit on OVR, make predictions, test overall accuracy.\n",
    "train_df, test_df = merged_df.randomSplit([0.8, 0.2], seed=42)\n",
    "ovr_model = ovr.fit(train_df)\n",
    "predictions = ovr_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "print(f\"\\nOne-Vs-Rest Test Accuracy = {evaluator.evaluate(predictions) * 100:.2f}%\\n\")\n",
    "\n",
    "# since pyspark's OneVsRest does not directly provide AUC or feature importances,\n",
    "# we will manually compute them for each binary classifier.\n",
    "\n",
    "# parties from labelindexer\n",
    "for i, party in enumerate(parties):\n",
    "    print(f\"\\n=== One-vs-Rest for: {party} vs. ALL ===\")\n",
    "\n",
    "    # get the corresponding binary model for the party\n",
    "    binary_model = ovr_model.models[i]\n",
    "\n",
    "    # create a binary test set for the current party (1 if current party, 0 if not)\n",
    "    test_binary = test_df.withColumn(\"binary_label\", when(col(\"label\") == i, 1).otherwise(0))\n",
    "\n",
    "    # make predictions for AUC evaluation and then compute AUC (area under ROC curve)\n",
    "    binary_preds = binary_model.transform(test_binary)\n",
    "    evaluator_bin = BinaryClassificationEvaluator(labelCol=\"binary_label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "    print(f\"Area Under ROC for {party} vs. All: {evaluator_bin.evaluate(binary_preds):.3f}\")\n",
    "\n",
    "    # extract feature importances\n",
    "    importances = binary_model.featureImportances\n",
    "    for j, feature_name in enumerate(feature_cols):\n",
    "        print(f\"  {feature_name}: {importances[j]:.4f}\")\n",
    "\n",
    "print(\"\\nDone\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:**\n",
    "\n",
    "The One-Vs-Rest model achieved an overall test accuracy of 78.04%, which is relatively strong. However, the Area Under ROC (AUC) scores vary significantly between parties, indicating that the model predicts some parties better than others.\n",
    "\n",
    "SP√ñ (AUC = 0.839) and √ñVP (AUC = 0.772) are well-predicted.\n",
    "\n",
    "FP√ñ (AUC = 0.633) is much weaker, closer to random guessing (0.5).\n",
    "\n",
    "This suggests that the features used (spending categories, turnout, education spending %) do not explain FP√ñ victories as well as they do for SP√ñ and √ñVP.\n",
    "\n",
    "\n",
    "Interpretations:\n",
    "\n",
    "**√ñVP vs. ALL (AUC = 0.772)**\n",
    "- The most influential features are extracurricular youth education (20.43%) and adult education (14.29%)\n",
    "\n",
    "**SP√ñ vs. ALL (AUC = 0.839)**\n",
    "- Extracurricular youth education (15.00%) and `Gesonderte_Verwaltung` i.e. administrative costs (14.70%)  are the strongest factors. Adult education (12.49%) is also important.\n",
    "\n",
    "Both of these parties are known for their stance supporting public infrastructure (including education) spending, so these results are expected.\n",
    "\n",
    "Voter turnout (8.10%) plays a slighly larger role for SP√ñ than for √ñVP (8.51%). This might indicate that both parties benefit from low or high turnouts, most likely from high.\n",
    "\n",
    "**FP√ñ vs. ALL (AUC = 0.633)**\n",
    "- The low AUC suggests that spending and voter turnout are not very good predictors of FP√ñ wins.\n",
    "- General education (13.63%) and the ratio of education spending to total spending (13.19%) are more relevant here, which is relatively unexpected given FP√ñ's right-wing, anti-establishment stance.\n",
    "- Turnout (10.78%) is more influential than for √ñVP/SP√ñ. This might indicate that FP√ñ benefits from either low or high turnouts.\n",
    "- The low AUC score is expected for FP√ñ, since the party benefits from other external factors not captured in this dataset, e.g. anti-establishment sentiment or sociocultural issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the analysis below is unrelated to our research topic, we will analyze the relationship between voter turnout and party wins out of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Turnout: 71.77%, Standard Deviation: 6.66%\n",
      "Party Win Distribution by Turnout Level:\n",
      "+-------------+----------------+-----+----------+----------+\n",
      "|Winning_Party|Turnout_Category|count|total_wins|Percentage|\n",
      "+-------------+----------------+-----+----------+----------+\n",
      "|          FPO|     Extreme Low|    5|        87|      5.75|\n",
      "|          FPO|   Moderate High|    3|        87|      3.45|\n",
      "|          FPO|    Moderate Low|   15|        87|     17.24|\n",
      "|          FPO|       Near Mean|   64|        87|     73.56|\n",
      "|          OVP|    Extreme High|   14|       911|      1.54|\n",
      "|          OVP|     Extreme Low|   28|       911|      3.07|\n",
      "|          OVP|   Moderate High|  136|       911|     14.93|\n",
      "|          OVP|    Moderate Low|  135|       911|     14.82|\n",
      "|          OVP|       Near Mean|  598|       911|     65.64|\n",
      "|          SPO|    Extreme High|    1|       242|      0.41|\n",
      "|          SPO|     Extreme Low|    4|       242|      1.65|\n",
      "|          SPO|   Moderate High|   43|       242|     17.77|\n",
      "|          SPO|    Moderate Low|   18|       242|      7.44|\n",
      "|          SPO|       Near Mean|  176|       242|     72.73|\n",
      "+-------------+----------------+-----+----------+----------+\n",
      "\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "merged_data = spark.read.csv(\"data/merged_data.csv\", header=True, inferSchema=True)\n",
    "merged_data = merged_data.withColumn(\"Wahlbeteiligung\", col(\"Wahlbeteiligung\").cast(\"float\"))\n",
    "\n",
    "# compute mean and stdev\n",
    "turnout_stats = merged_data.select(\n",
    "    F.mean(\"Wahlbeteiligung\").alias(\"mean_turnout\"),\n",
    "    F.stddev(\"Wahlbeteiligung\").alias(\"stddev_turnout\")\n",
    ").collect()[0]\n",
    "\n",
    "mean_turnout = turnout_stats[\"mean_turnout\"]\n",
    "stddev_turnout = turnout_stats[\"stddev_turnout\"]\n",
    "\n",
    "print(f\"Mean Turnout: {mean_turnout:.2f}%, Standard Deviation: {stddev_turnout:.2f}%\")\n",
    "\n",
    "# compute deviation from mean\n",
    "merged_data = merged_data.withColumn(\"Turnout_Deviation\", (col(\"Wahlbeteiligung\") - mean_turnout).cast(\"float\"))\n",
    "\n",
    "# assign turnout categories based on st dev\n",
    "# <=1œÉ near median\n",
    "# 1<œÉ<=2 moderate high/low (depending on the direction of the deviance of turnout from mean)\n",
    "# 2<œÉ extreme high/low\n",
    "\n",
    "merged_data = merged_data.withColumn(\n",
    "    \"Turnout_Category\",\n",
    "    when(F.abs(col(\"Turnout_Deviation\")) <= stddev_turnout, \"Near Mean\")\n",
    "    .when((col(\"Turnout_Deviation\") > stddev_turnout) & (col(\"Turnout_Deviation\") <= 2 * stddev_turnout), \"Moderate High\")\n",
    "    .when((col(\"Turnout_Deviation\") < -stddev_turnout) & (col(\"Turnout_Deviation\") >= -2 * stddev_turnout), \"Moderate Low\")\n",
    "    .when(col(\"Turnout_Deviation\") > 2 * stddev_turnout, \"Extreme High\")\n",
    "    .otherwise(\"Extreme Low\")\n",
    ")\n",
    "\n",
    "# count party wins across turnout categories\n",
    "party_wins = merged_data.groupBy(\"Winning_Party\", \"Turnout_Category\").count()\n",
    "\n",
    "# calculate total wins per party and calculate turnout category wins / total wins\n",
    "total_wins_per_party = party_wins.groupBy(\"Winning_Party\").agg(F.sum(\"count\").alias(\"total_wins\"))\n",
    "party_wins = party_wins.join(total_wins_per_party, on=\"Winning_Party\", how=\"left\") \\\n",
    "                       .withColumn(\"Percentage\", F.round((col(\"count\") / col(\"total_wins\")) * 100, 2))\n",
    "\n",
    "print(\"Party Win Distribution by Turnout Level:\")\n",
    "party_wins.orderBy(\"Winning_Party\", \"Turnout_Category\").show()\n",
    "\n",
    "print(\"\\nDone\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion**\n",
    "\n",
    "As suspected, voter turnout does affect electoral success for different parties.\n",
    "\n",
    "**√ñVP (Austrian People's Party)**\n",
    "\n",
    "√ñVP, a center-right, traditionally conservative party, performs strongly in Near Median turnout areas (65.64%), while its success in Moderate High (14.93%) and Moderate Low (14.82%) turnout areas is nearly identical.\n",
    "\n",
    "Interestingly, √ñVP does not perform significantly better in extreme turnout scenarios, with only 1.54% of wins in Extreme High turnout and 3.07% in Extreme Low turnout.\n",
    "\n",
    "This suggests that √ñVP‚Äôs voter base is relatively stable across different turnout levels, reinforcing the idea that the party appeals to an established, consistent voterbase, irrespective of voter turnout.\n",
    "\n",
    "**SP√ñ (Social Democratic Party of Austria)**\n",
    "\n",
    "SP√ñ, a center-left party, also sees most of its victories in Near Median turnout areas (72.73%), but with relatively strong presence in Moderate High turnout (17.77%).\n",
    "\n",
    "SP√ñ underperforms in Extreme High turnout (0.41%) and Extreme Low turnout (1.65%), indicating that it does not significantly benefit from electoral volatility.\n",
    "\n",
    "The party's strong relative performance in Moderate High turnout areas suggests that higher turnout slightly favors SP√ñ, which aligns with expectations that left-leaning parties benefit when voter mobilization efforts succeed.\n",
    "\n",
    "However, SP√ñ's lower success in Moderate Low (7.44%) and Extreme Low turnout areas (1.65%) suggests that it struggles when voter participation drops significantly.\n",
    "\n",
    "**FP√ñ (Freedom Party of Austria)**\n",
    "\n",
    "FP√ñ, a right-wing populist and nationalist party, shows an overwhelming concentration of wins in Near Median turnout areas (73.56%), but its second-highest win category is Moderate Low turnout (17.24%). It also performs well in Extreme Low (5.75%) turnouts compared to other parties. Moderate High turnout (3.45%) is weak and it has no wins in Extreme High turnout areas.\n",
    "\n",
    "Unlike √ñVP, which remains relatively stable across turnout conditions, FP√ñ shows a clear bias toward lower turnout conditions, reinforcing the idea that it benefits more from disengaged voters than from high-turnout mobilization efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ETL (Extract Transform Load) Script for getting the data. We first scrape the data using bs4. See `webscraper.py`. Afterwards we get the election data and process them and merge them together into a single csv. \n",
    "\n",
    "**TODO how did we get the gemeinden list?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first run the scraping script. Since the script takes a long time (if we assume an HTTP request takes 0.5 seconds and parsing/extracting takes 0.1 seconds, the scraping would take almost an hour), we do not run the script right now for convenience. However, the data from a scrape (date accessed: 2/4/2025) is saved in `data/Bildungsausgaben_Gemeinden_Oberösterreich_data_2007_bis_2019.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run webscraper.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we get the data for the elections. We will use spark from now on, to ensure scalability.\n",
    "\n",
    "**TODO how did we get election data**\n",
    "\n",
    "Since the election data was originally provided in an inconsistent XLS format—with irregular merged cells, random empty lines, and other structural issues—we manually formatted the data to ensure consistency and saved it as an XLSX file for better usability.\n",
    "\n",
    "Since we had only 5 XLS files for election data, this was the easiest option. However, this can be automated using a script for formatting if we were to automate and scale this project. However, this is out of scope for now.\n",
    "\n",
    "Afterward, we used a small script to convert these XLSX files to CSV. Given the limited number of election datasets and the straightforward nature of the conversion, the added complexity and overhead of Spark are not justified. Therefore, we will use Pandas for this conversion. However, if scalability in this specific conversion becomes a concern, the script can be rewritten to use PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted: OÖ 2017.xlsx (Sheet: Stimmen) -> /home/jovyan/DP2/Project/data/OÖ_2017_Stimmen.csv\n",
      "Converted: OÖ 2017.xlsx (Sheet: Mandate) -> /home/jovyan/DP2/Project/data/OÖ_2017_Mandate.csv\n",
      "Converted: OÖ 2019.xlsx (Sheet: Stimmen) -> /home/jovyan/DP2/Project/data/OÖ_2019_Stimmen.csv\n",
      "Converted: OÖ 2019.xlsx (Sheet: Mandate) -> /home/jovyan/DP2/Project/data/OÖ_2019_Mandate.csv\n",
      "Converted: OÖ 2024.xlsx (Sheet: Stimmen) -> /home/jovyan/DP2/Project/data/OÖ_2024_Stimmen.csv\n",
      "Converted: OÖ 2024.xlsx (Sheet: Mandate) -> /home/jovyan/DP2/Project/data/OÖ_2024_Mandate.csv\n",
      "Converted: OÖ 2008.xlsx (Sheet: Stimmen) -> /home/jovyan/DP2/Project/data/OÖ_2008_Stimmen.csv\n",
      "Converted: OÖ 2008.xlsx (Sheet: Mandate) -> /home/jovyan/DP2/Project/data/OÖ_2008_Mandate.csv\n",
      "Converted: OÖ 2013.xlsx (Sheet: Stimmen) -> /home/jovyan/DP2/Project/data/OÖ_2013_Stimmen.csv\n",
      "Converted: OÖ 2013.xlsx (Sheet: Mandate) -> /home/jovyan/DP2/Project/data/OÖ_2013_Mandate.csv\n",
      "Converted: OÖ 2013.xlsx (Sheet: whlsortgemnr) -> /home/jovyan/DP2/Project/data/OÖ_2013_whlsortgemnr.csv\n",
      "Converted: OÖ 2013.xlsx (Sheet: MandateData) -> /home/jovyan/DP2/Project/data/OÖ_2013_MandateData.csv\n",
      "Converted: OÖ 2013.xlsx (Sheet: WAHLDAT) -> /home/jovyan/DP2/Project/data/OÖ_2013_WAHLDAT.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "# Get all .xlsx files in the data directory\n",
    "xlsx_files = [f for f in os.listdir(data_dir) if f.endswith(\".xlsx\")]\n",
    "\n",
    "for file in xlsx_files:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    try:\n",
    "        df = pd.read_excel(file_path, sheet_name=None)\n",
    "        for sheet_name, sheet_df in df.items():\n",
    "            # Replace spaces with underscores in filename\n",
    "            sanitized_filename = os.path.splitext(file)[0].replace(\" \", \"_\")\n",
    "            sanitized_sheet_name = sheet_name.replace(\" \", \"_\")\n",
    "            csv_filename = os.path.join(data_dir, f\"{sanitized_filename}_{sanitized_sheet_name}.csv\")\n",
    "\n",
    "            sheet_df.to_csv(csv_filename, index=False, sep=\",\", encoding=\"latin1\")\n",
    "            print(f\"Converted: {file} (Sheet: {sheet_name}) -> {csv_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting {file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OÖ 2013.xlsx contained 3 (hidden) sheets, which upon manual inspection, are not relevant and/or redundant for us. Therefore we remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm data/OÖ_2013_whlsortgemnr.csv\n",
    "!rm data/OÖ_2013_MandateData.csv\n",
    "!rm data/OÖ_2013_WAHLDAT.csv"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We now start with extracting the data from the CSV files. First, we initialize spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, when, lit, expr\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "from unidecode import unidecode # https://pypi.org/project/Unidecode/\n",
    "# init spark\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"MunicipalSpendingAndElectionAnalysis\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import the data from the CSV files and clean them. We start with the spendings data:\n",
    "- we remove encoding issues we noticed upon manual inspection\n",
    "- we rename categories for readability and cast their type\n",
    "- we aggregate the spending data.\n",
    "    - we do this becuase the data is currently broken down into subcategories (e.g. spendings on preschool education, on physical education etc.)\n",
    "    - for some of the analysis we will use the total spending per municipality\n",
    "- we slugify using the helper function the municipality names so we can process them easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# helper function\n",
    "def standardize_municipality(col_name):\n",
    "    \"\"\"lowercase, trim, replace spaces with dashes (i.e. slugify)\"\"\"\n",
    "    return F.regexp_replace(F.lower(F.trim(F.col(col_name))), \" \", \"-\")\n",
    "\n",
    "\n",
    "# --- read & transform municipal spending data ---\n",
    "spending_csv = \"data/Bildungsausgaben_Gemeinden_Oberösterreich_data_2007_bis_2019.csv\"\n",
    "df_spending = (\n",
    "    spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"encoding\", \"UTF-8\")\n",
    "    .csv(spending_csv)\n",
    ")\n",
    "\n",
    "# rename cols\n",
    "df_spending = (df_spending\n",
    "    .withColumnRenamed(\"Gemeinde\", \"Municipality\")\n",
    "    .withColumnRenamed(\"Year\", \"Year\")\n",
    "    .withColumnRenamed(\"Abschnitt\", \"Category\")\n",
    "    .withColumnRenamed(\"Betrag in Euro\", \"Spending\")\n",
    ")\n",
    "\n",
    "# replace chars in category\n",
    "df_spending = df_spending.withColumn(\n",
    "    \"Category\",\n",
    "    regexp_replace(col(\"Category\"), \"FĂ¶rderung\", \"Förderung\")\n",
    ")\n",
    "df_spending = df_spending.withColumn(\n",
    "    \"Category\",\n",
    "    regexp_replace(col(\"Category\"), \"Sport und auĂźerschulische Leibeserziehung\",\n",
    "                   \"Sport und außerschulische Leibeserziehung\")\n",
    ")\n",
    "\n",
    "# cast cols\n",
    "df_spending = df_spending.withColumn(\"Year\", col(\"Year\").cast(\"int\"))\n",
    "df_spending = df_spending.withColumn(\"Spending\", col(\"Spending\").cast(\"float\"))\n",
    "\n",
    "# aggregate spending\n",
    "df_spending_agg = (\n",
    "    df_spending\n",
    "    .groupBy(\"Municipality\", \"Year\")\n",
    "    .agg(F.sum(\"Spending\").alias(\"Spending_Summe\"))\n",
    ")\n",
    "\n",
    "# add lowercase municipality\n",
    "df_spending_agg = df_spending_agg.withColumn(\"Municipality_Lowercase\", standardize_municipality(\"Municipality\"))\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we process the election data for the available years. We initialize an empty DataFrame with predefined columns to store the cleaned data. The predefined columns are consistent amongst Austrian election datasets. Therefore, this script would presumably work for any election data across Austria. For including other countries, at first, we would need to standardize their election data to this format.\n",
    "\n",
    "We read the CSV into Spark, standardize column names, and identify party vote columns. We convert party vote counts to integers and determine the winning party for each municipality, which we will use for predictive analysis later on. We also add a slugified version of the municipality name for consistency.\n",
    "\n",
    "Once cleaned, the election data is merged into a single DataFrame and joined with the spending data using slugified municipality name and year. Since spending data only covers 2007–2019, elections from 2024 are automatically excluded. However, upon adding recent spending data, 2024 would be automatically included without needing manual modification to this script. Finally, we save the merged dataset as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for 2008\n",
      "Processing data for 2013\n",
      "Processing data for 2017\n",
      "Processing data for 2019\n",
      "Processing data for 2024\n",
      "Merged data sample:\n",
      "+----------------------+----+---------------+------------------------------+---------------+------------------+----------------+---------------+-------------+-------------------+\n",
      "|Municipality_Lowercase|Year|Municipality_ID|Municipality_Name             |Wahlberechtigte|abgegebene_Stimmen|gueltige_Stimmen|Wahlbeteiligung|Winning_Party|Spending_Summe     |\n",
      "+----------------------+----+---------------+------------------------------+---------------+------------------+----------------+---------------+-------------+-------------------+\n",
      "|linz                  |2008|40101          |Linz                          |142125         |96209             |94496           |67.69          |SPO          |1.203534975E8      |\n",
      "|steyr                 |2008|40201          |Steyr                         |28962          |20765             |20335           |71.7           |SPO          |3.377888836328125E7|\n",
      "|wels                  |2008|40301          |Wels                          |40994          |28803             |28288           |70.26          |SPO          |5.50799846875E7    |\n",
      "|aspach                |2008|40402          |Aspach                        |1869           |1390              |1346            |74.37          |OVP          |1433794.4849853516 |\n",
      "|auerbach              |2008|40403          |Auerbach                      |411            |310               |301             |75.43          |OVP          |160001.2024230957  |\n",
      "+----------------------+----+---------------+------------------------------+---------------+------------------+----------------+---------------+-------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- read & transform election data ---\n",
    "election_yrs = [2008, 2013, 2017, 2019, 2024]\n",
    "all_election_df = spark.createDataFrame([], schema=\"Year INT, Municipality_ID STRING, Municipality_Name STRING, Wahlberechtigte INT, abgegebene_Stimmen INT, gueltige_Stimmen INT, Wahlbeteiligung STRING, Winning_Party STRING, Municipality_Lowercase STRING\")\n",
    "\n",
    "for year in election_yrs:\n",
    "    print(f\"Processing data for {year}\")\n",
    "    votes_file = f\"data/OÖ_{year}_Stimmen.csv\"\n",
    "\n",
    "    # skip if file missing\n",
    "    if not os.path.exists(votes_file):\n",
    "        print(f\"[WARNING] Missing CSV(s) for year {year} -> Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # read csv\n",
    "    votes_df = (\n",
    "        spark.read\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .option(\"encoding\", \"latin1\")\n",
    "        .csv(votes_file)\n",
    "    )\n",
    "\n",
    "    # normalize col names\n",
    "    renamed_columns = {c: unidecode(c.strip()).replace(\" \", \"_\") for c in votes_df.columns}\n",
    "    votes_df = votes_df.selectExpr(*[f\"`{old}` as `{new}`\" for old, new in renamed_columns.items()])\n",
    "    \n",
    "    # identify party cols\n",
    "    party_columns = [c for c in votes_df.columns if c not in {\n",
    "        \"Nr.\", \"Name\", \"Wahlberechtigte\", \"abgegeb._Stimmen\", \"gultige\", \"Wahlbet.\", \"ungultige\"\n",
    "    }]\n",
    "\n",
    "    # rename cols\n",
    "    votes_df = votes_df.withColumnRenamed(\"Nr.\", \"Municipality_ID\")\n",
    "    votes_df = votes_df.withColumnRenamed(\"Name\", \"Municipality_Name\")\n",
    "    votes_df = votes_df.withColumnRenamed(\"Wahlberechtigte\", \"Wahlberechtigte\")\n",
    "    votes_df = votes_df.withColumnRenamed(\"abgegeb._Stimmen\", \"abgegebene_Stimmen\")\n",
    "    votes_df = votes_df.withColumnRenamed(\"gultige\", \"gueltige_Stimmen\")\n",
    "    votes_df = votes_df.withColumnRenamed(\"Wahlbet.\", \"Wahlbeteiligung\")\n",
    "\n",
    "    # cast party cols to int\n",
    "    for party_col in party_columns:\n",
    "        votes_df = votes_df.withColumn(f\"votes_{party_col}\", col(party_col).cast(\"int\"))\n",
    "\n",
    "    # determine win party\n",
    "    vote_cols = [F.coalesce(col(f\"votes_{party_col}\"), lit(0)) for party_col in party_columns]\n",
    "    votes_df = votes_df.withColumn(\"max_votes\", F.greatest(*vote_cols))\n",
    "\n",
    "    winning_party_cases = [when(col(\"max_votes\") == col(f\"votes_{party_col}\"), lit(party_col)) for party_col in party_columns]\n",
    "    votes_df = votes_df.withColumn(\"Winning_Party\", F.coalesce(*winning_party_cases))\n",
    "\n",
    "    # drop temp cols\n",
    "    for party_col in party_columns:\n",
    "        votes_df = votes_df.drop(f\"votes_{party_col}\")\n",
    "\n",
    "    votes_df = votes_df.drop(\"max_votes\")\n",
    "\n",
    "    # add year & municipality lowercase\n",
    "    votes_df = votes_df.withColumn(\"Year\", lit(year))\n",
    "    votes_df = votes_df.withColumn(\"Municipality_Lowercase\", standardize_municipality(\"Municipality_Name\"))\n",
    "    # filter for numeric municipality ids\n",
    "    votes_df = votes_df.filter(col(\"Municipality_ID\").rlike(\"^[0-9]+$\"))\n",
    "\n",
    "    # add to main df\n",
    "    all_election_df = all_election_df.unionByName(votes_df.select(\n",
    "        \"Year\", \"Municipality_ID\", \"Municipality_Name\", \"Wahlberechtigte\",\n",
    "        \"abgegebene_Stimmen\", \"gueltige_Stimmen\", \"Wahlbeteiligung\", \"Winning_Party\", \"Municipality_Lowercase\"\n",
    "    ))\n",
    "\n",
    "# --- merging and output ---\n",
    "# join dfs\n",
    "merged_df = all_election_df.join(df_spending_agg, on=[\"Municipality_Lowercase\", \"Year\"], how=\"inner\").drop(\"Municipality\")\n",
    "# write to csv\n",
    "merged_df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").option(\"encoding\", \"latin1\").csv(\"data/merged_data.csv\")\n",
    "print(\"Merged data sample:\")\n",
    "merged_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
